{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": 222, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['MGH-100P/features_Case1_seg12.mat.csv', 'MGH-100P/new_features_Case101_seg1.mat.csv', 'MGH-100P/new_features_Case101_seg2.mat.csv', 'MGH-100P/new_features_Case101_seg4.mat.csv', 'MGH-100P/new_features_Case101_seg6.mat.csv', 'MGH-100P/new_features_Case102_seg1.mat.csv', 'MGH-100P/new_features_Case102_seg3.mat.csv', 'MGH-100P/new_features_Case102_seg4.mat.csv', 'MGH-100P/new_features_Case102_seg6.mat.csv', 'MGH-100P/new_features_Case103_seg6.mat.csv', 'MGH-100P/new_features_Case103_seg9.mat.csv', 'MGH-100P/new_features_Case104_seg2.mat.csv', 'MGH-100P/new_features_Case104_seg4.mat.csv', 'MGH-100P/new_features_Case105_seg1.mat.csv', 'MGH-100P/new_features_Case105_seg2.mat.csv', 'MGH-100P/new_features_Case105_seg4.mat.csv', 'MGH-100P/new_features_Case105_seg5.mat.csv', 'MGH-100P/new_features_Case106_seg1.mat.csv', 'MGH-100P/new_features_Case106_seg3.mat.csv', 'MGH-100P/new_features_Case107_seg10.mat.csv', 'MGH-100P/new_features_Case107_seg1.mat.csv', 'MGH-100P/new_features_Case107_seg2.mat.csv', 'MGH-100P/new_features_Case107_seg7.mat.csv', 'MGH-100P/new_features_Case107_seg8.mat.csv', 'MGH-100P/new_features_Case108_seg10.mat.csv', 'MGH-100P/new_features_Case108_seg15.mat.csv', 'MGH-100P/new_features_Case108_seg5.mat.csv', 'MGH-100P/new_features_Case109_seg1.mat.csv', 'MGH-100P/new_features_Case109_seg2.mat.csv', 'MGH-100P/new_features_Case109_seg5.mat.csv', 'MGH-100P/new_features_Case109_seg6.mat.csv', 'MGH-100P/new_features_Case10_seg1.mat.csv', 'MGH-100P/new_features_Case10_seg3.mat.csv', 'MGH-100P/new_features_Case10_seg5.mat.csv', 'MGH-100P/new_features_Case10_seg6.mat.csv', 'MGH-100P/new_features_Case10_seg7.mat.csv', 'MGH-100P/new_features_Case10_seg9.mat.csv', 'MGH-100P/new_features_Case110_seg1.mat.csv', 'MGH-100P/new_features_Case110_seg2.mat.csv', 'MGH-100P/new_features_Case110_seg3.mat.csv', 'MGH-100P/new_features_Case110_seg4.mat.csv', 'MGH-100P/new_features_Case111_seg1.mat.csv', 'MGH-100P/new_features_Case111_seg2.mat.csv', 'MGH-100P/new_features_Case111_seg5.mat.csv', 'MGH-100P/new_features_Case111_seg7.mat.csv', 'MGH-100P/new_features_Case112_seg5.mat.csv', 'MGH-100P/new_features_Case112_seg6.mat.csv', 'MGH-100P/new_features_Case112_seg8.mat.csv', 'MGH-100P/new_features_Case113_seg10.mat.csv', 'MGH-100P/new_features_Case113_seg12.mat.csv', 'MGH-100P/new_features_Case113_seg3.mat.csv', 'MGH-100P/new_features_Case113_seg9.mat.csv', 'MGH-100P/new_features_Case114_seg2.mat.csv', 'MGH-100P/new_features_Case115_seg4.mat.csv', 'MGH-100P/new_features_Case116_seg1.mat.csv', 'MGH-100P/new_features_Case116_seg4.mat.csv', 'MGH-100P/new_features_Case116_seg7.mat.csv', 'MGH-100P/new_features_Case117_seg2.mat.csv', 'MGH-100P/new_features_Case118_seg1.mat.csv', 'MGH-100P/new_features_Case118_seg3.mat.csv', 'MGH-100P/new_features_Case118_seg4.mat.csv', 'MGH-100P/new_features_Case118_seg6.mat.csv', 'MGH-100P/new_features_Case119_seg2.mat.csv', 'MGH-100P/new_features_Case119_seg3.mat.csv', 'MGH-100P/new_features_Case119_seg5.mat.csv', 'MGH-100P/new_features_Case11_seg4.mat.csv', 'MGH-100P/new_features_Case11_seg7.mat.csv', 'MGH-100P/new_features_Case120_seg1.mat.csv', 'MGH-100P/new_features_Case120_seg2.mat.csv', 'MGH-100P/new_features_Case121_seg6.mat.csv', 'MGH-100P/new_features_Case122_seg1.mat.csv', 'MGH-100P/new_features_Case122_seg4.mat.csv', 'MGH-100P/new_features_Case122_seg7.mat.csv', 'MGH-100P/new_features_Case123_seg6.mat.csv', 'MGH-100P/new_features_Case124_seg4.mat.csv', 'MGH-100P/new_features_Case125_seg6.mat.csv', 'MGH-100P/new_features_Case126_seg3.mat.csv', 'MGH-100P/new_features_Case126_seg4.mat.csv', 'MGH-100P/new_features_Case127_seg3.mat.csv', 'MGH-100P/new_features_Case128_seg5.mat.csv', 'MGH-100P/new_features_Case129_seg8.mat.csv', 'MGH-100P/new_features_Case12_seg2.mat.csv', 'MGH-100P/new_features_Case130_seg3.mat.csv', 'MGH-100P/new_features_Case130_seg6.mat.csv', 'MGH-100P/new_features_Case13_seg3.mat.csv', 'MGH-100P/new_features_Case13_seg7.mat.csv', 'MGH-100P/new_features_Case15_seg7.mat.csv', 'MGH-100P/new_features_Case16_seg3.mat.csv', 'MGH-100P/new_features_Case17_seg3.mat.csv', 'MGH-100P/new_features_Case17_seg7.mat.csv', 'MGH-100P/new_features_Case18_seg4.mat.csv', 'MGH-100P/new_features_Case1_seg12.mat.csv', 'MGH-100P/new_features_Case2_seg1.mat.csv', 'MGH-100P/new_features_Case31_seg2.mat.csv', 'MGH-100P/new_features_Case31_seg4.mat.csv', 'MGH-100P/new_features_Case32_seg1.mat.csv', 'MGH-100P/new_features_Case33_seg10.mat.csv', 'MGH-100P/new_features_Case33_seg5.mat.csv', 'MGH-100P/new_features_Case34_seg3.mat.csv', 'MGH-100P/new_features_Case35_seg8.mat.csv', 'MGH-100P/new_features_Case36_seg6.mat.csv', 'MGH-100P/new_features_Case38_seg4.mat.csv', 'MGH-100P/new_features_Case39_seg4.mat.csv', 'MGH-100P/new_features_Case42_seg3.mat.csv', 'MGH-100P/new_features_Case43_seg2.mat.csv', 'MGH-100P/new_features_Case44_seg3.mat.csv', 'MGH-100P/new_features_Case45_seg7.mat.csv', 'MGH-100P/new_features_Case49_seg9.mat.csv', 'MGH-100P/new_features_Case4_seg1.mat.csv', 'MGH-100P/new_features_Case50_seg4.mat.csv', 'MGH-100P/new_features_Case50_seg6.mat.csv', 'MGH-100P/new_features_Case51_seg2.mat.csv', 'MGH-100P/new_features_Case51_seg4.mat.csv', 'MGH-100P/new_features_Case51_seg6.mat.csv', 'MGH-100P/new_features_Case52_seg1.mat.csv', 'MGH-100P/new_features_Case52_seg4.mat.csv', 'MGH-100P/new_features_Case53_seg3.mat.csv', 'MGH-100P/new_features_Case53_seg4.mat.csv', 'MGH-100P/new_features_Case53_seg7.mat.csv', 'MGH-100P/new_features_Case53_seg9.mat.csv', 'MGH-100P/new_features_Case54_seg3.mat.csv', 'MGH-100P/new_features_Case55_seg2.mat.csv', 'MGH-100P/new_features_Case56_seg1.mat.csv', 'MGH-100P/new_features_Case56_seg2.mat.csv', 'MGH-100P/new_features_Case56_seg3.mat.csv', 'MGH-100P/new_features_Case56_seg4.mat.csv', 'MGH-100P/new_features_Case57_seg3.mat.csv', 'MGH-100P/new_features_Case57_seg6.mat.csv', 'MGH-100P/new_features_Case58_seg1.mat.csv', 'MGH-100P/new_features_Case58_seg3.mat.csv', 'MGH-100P/new_features_Case59_seg1.mat.csv', 'MGH-100P/new_features_Case59_seg2.mat.csv', 'MGH-100P/new_features_Case59_seg3.mat.csv', 'MGH-100P/new_features_Case5_seg5.mat.csv', 'MGH-100P/new_features_Case5_seg6.mat.csv', 'MGH-100P/new_features_Case60_seg1.mat.csv', 'MGH-100P/new_features_Case72_seg2.mat.csv', 'MGH-100P/new_features_Case73_seg4.mat.csv', 'MGH-100P/new_features_Case75_seg8.mat.csv', 'MGH-100P/new_features_Case78_seg10.mat.csv', 'MGH-100P/new_features_Case78_seg6.mat.csv', 'MGH-100P/new_features_Case89_seg3.mat.csv', 'MGH-100P/new_features_Case8_seg6.mat.csv']\n143\n"
                }
            ], 
            "source": "# output bench time\n!find MGH-100P/*.csv -mmin -60 -exec ls -gohd {} \\;\nlfeats=!find MGH-100P/*.csv\nlfeats=list(lfeats)\nprint(lfeats)\nprint(len(lfeats))"
        }, 
        {
            "execution_count": 179, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "1\n[\"find: missing argument to `-exec'\"]\n"
                }
            ], 
            "source": "#!ls -l Case5\n#!ls -l pid\n#lsub=!find Case*/*.mat -size -100000\nlsub=!find Case*/*.mat -type f -size +300M -size -600M -exec ls {}\\;\n\nprint(len(lsub))\nprint(lsub)\n#!ls -l Case102/Case102_seg3.mat"
        }, 
        {
            "execution_count": 93, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "sc.addPyFile(\"fcn_shannon_entro.py\")\nsc.addPyFile(\"full_pipeline.py\")\nsc.addPyFile(\"compute_spectrogram_sunhaoqi.py\")\nsc.addPyFile(\"get_features.py\")\nsc.addPyFile(\"runtime.py\")\nsc.addPyFile(\"mtspec.py\")"
        }, 
        {
            "execution_count": 167, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['Case101/Case101_seg1.mat', 'Case101/Case101_seg2.mat', 'Case101/Case101_seg3.mat', 'Case101/Case101_seg4.mat', 'Case101/Case101_seg5.mat', 'Case101/Case101_seg6.mat', 'Case102/Case102_seg1.mat', 'Case102/Case102_seg2.mat', 'Case102/Case102_seg3.mat', 'Case102/Case102_seg4.mat', 'Case102/Case102_seg5.mat', 'Case102/Case102_seg6.mat', 'Case103/Case103_seg1.mat', 'Case103/Case103_seg2.mat', 'Case103/Case103_seg3.mat', 'Case103/Case103_seg4.mat', 'Case103/Case103_seg5.mat', 'Case103/Case103_seg6.mat', 'Case103/Case103_seg8.mat', 'Case103/Case103_seg9.mat', 'Case104/Case104_seg1.mat', 'Case104/Case104_seg2.mat', 'Case104/Case104_seg3.mat', 'Case104/Case104_seg4.mat', 'Case105/Case105_seg1.mat', 'Case105/Case105_seg2.mat', 'Case105/Case105_seg3.mat', 'Case105/Case105_seg4.mat', 'Case105/Case105_seg5.mat', 'Case105/Case105_seg6.mat', 'Case105/Case105_seg7.mat', 'Case106/Case106_seg1.mat', 'Case106/Case106_seg2.mat', 'Case106/Case106_seg3.mat', 'Case107/Case107_seg10.mat', 'Case107/Case107_seg11.mat', 'Case107/Case107_seg1.mat', 'Case107/Case107_seg2.mat', 'Case107/Case107_seg3.mat', 'Case107/Case107_seg4.mat', 'Case107/Case107_seg5.mat', 'Case107/Case107_seg6.mat', 'Case107/Case107_seg7.mat', 'Case107/Case107_seg8.mat', 'Case107/Case107_seg9.mat', 'Case108/Case108_seg10.mat', 'Case108/Case108_seg11.mat', 'Case108/Case108_seg12.mat', 'Case108/Case108_seg13.mat', 'Case108/Case108_seg14.mat', 'Case108/Case108_seg15.mat', 'Case108/Case108_seg1.mat', 'Case108/Case108_seg2.mat', 'Case108/Case108_seg3.mat', 'Case108/Case108_seg4.mat', 'Case108/Case108_seg5.mat', 'Case108/Case108_seg6.mat', 'Case108/Case108_seg7.mat', 'Case108/Case108_seg8.mat', 'Case108/Case108_seg9.mat', 'Case109/Case109_seg1.mat', 'Case109/Case109_seg2.mat', 'Case109/Case109_seg3.mat', 'Case109/Case109_seg4.mat', 'Case109/Case109_seg5.mat', 'Case109/Case109_seg6.mat', 'Case10/Case10_seg10.mat', 'Case10/Case10_seg1.mat', 'Case10/Case10_seg2.mat', 'Case10/Case10_seg3.mat', 'Case10/Case10_seg4.mat', 'Case10/Case10_seg5.mat', 'Case10/Case10_seg6.mat', 'Case10/Case10_seg7.mat', 'Case10/Case10_seg8.mat', 'Case10/Case10_seg9.mat']\n76\n"
                }
            ], 
            "source": "lfiles=!ls Case10*/*.mat\n\n#lfiles=list(lfiles)\nlsub=lfiles\nprint(lsub)\nprint(len(lsub))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "num part 59\nnum data 76\nnum exec 59\ndata ['Case101/Case101_seg1.mat', 'Case101/Case101_seg2.mat', 'Case101/Case101_seg3.mat', 'Case101/Case101_seg4.mat', 'Case101/Case101_seg5.mat', 'Case101/Case101_seg6.mat', 'Case102/Case102_seg1.mat', 'Case102/Case102_seg2.mat', 'Case102/Case102_seg3.mat', 'Case102/Case102_seg4.mat', 'Case102/Case102_seg5.mat', 'Case102/Case102_seg6.mat', 'Case103/Case103_seg1.mat', 'Case103/Case103_seg2.mat', 'Case103/Case103_seg3.mat', 'Case103/Case103_seg4.mat', 'Case103/Case103_seg5.mat', 'Case103/Case103_seg6.mat', 'Case103/Case103_seg8.mat', 'Case103/Case103_seg9.mat', 'Case104/Case104_seg1.mat', 'Case104/Case104_seg2.mat', 'Case104/Case104_seg3.mat', 'Case104/Case104_seg4.mat', 'Case105/Case105_seg1.mat', 'Case105/Case105_seg2.mat', 'Case105/Case105_seg3.mat', 'Case105/Case105_seg4.mat', 'Case105/Case105_seg5.mat', 'Case105/Case105_seg6.mat', 'Case105/Case105_seg7.mat', 'Case106/Case106_seg1.mat', 'Case106/Case106_seg2.mat', 'Case106/Case106_seg3.mat', 'Case107/Case107_seg10.mat', 'Case107/Case107_seg11.mat', 'Case107/Case107_seg1.mat', 'Case107/Case107_seg2.mat', 'Case107/Case107_seg3.mat', 'Case107/Case107_seg4.mat', 'Case107/Case107_seg5.mat', 'Case107/Case107_seg6.mat', 'Case107/Case107_seg7.mat', 'Case107/Case107_seg8.mat', 'Case107/Case107_seg9.mat', 'Case108/Case108_seg10.mat', 'Case108/Case108_seg11.mat', 'Case108/Case108_seg12.mat', 'Case108/Case108_seg13.mat', 'Case108/Case108_seg14.mat', 'Case108/Case108_seg15.mat', 'Case108/Case108_seg1.mat', 'Case108/Case108_seg2.mat', 'Case108/Case108_seg3.mat', 'Case108/Case108_seg4.mat', 'Case108/Case108_seg5.mat', 'Case108/Case108_seg6.mat', 'Case108/Case108_seg7.mat', 'Case108/Case108_seg8.mat', 'Case108/Case108_seg9.mat', 'Case109/Case109_seg1.mat', 'Case109/Case109_seg2.mat', 'Case109/Case109_seg3.mat', 'Case109/Case109_seg4.mat', 'Case109/Case109_seg5.mat', 'Case109/Case109_seg6.mat', 'Case10/Case10_seg10.mat', 'Case10/Case10_seg1.mat', 'Case10/Case10_seg2.mat', 'Case10/Case10_seg3.mat', 'Case10/Case10_seg4.mat', 'Case10/Case10_seg5.mat', 'Case10/Case10_seg6.mat', 'Case10/Case10_seg7.mat', 'Case10/Case10_seg8.mat', 'Case10/Case10_seg9.mat']\n"
                }
            ], 
            "source": "import itertools\nimport os\nimport numpy as np\nimport hdf5storage\nimport spectrum\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom numpy.core.defchararray import lower\nfrom numpy import floor\nimport swiftclient\nimport pandas as pd\n#from compute_spectrogram_sunhaoqi import mtspecgram_shq\nfrom get_features import get_features\nfrom scipy.signal import butter, filtfilt, lfilter\nfrom runtime import cellarray, matlabarray, cat, cell, arange, size, disp, find, dot, copy, length, Dict2Obj,catstr\nimport scipy\n\nfrom mtspec import mt_spectrogram\n\ndef filtering_data(temp,Fs):\n    fnyq=Fs / 2  # nyquist frequency\n    fc_high=50\n    fc_low=0.5        \n    b, a = butter(6, [fc_low / fnyq, fc_high / fnyq], 'bandpass')\n    out=filtfilt(b,a,temp,padtype='odd',padlen=3*(np.amax((len(a), len(b))) - 1))\n    return out\n\n\ndef mockFunc(elt):\n    try:\n        #d=dict(sc._conf.getAll())\n        #print(d['spark.executor.id'])\n        \n        absdir='/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/'\n        elt=absdir+elt\n        dataset=os.path.getsize(elt)\n        matfile = hdf5storage.loadmat(elt)\n        \n      \n        Fs = cellarray(int(np.round(matfile['Fs'])))\n        start_time = cellarray(matfile['start_time'])\n        channels = cellarray(matfile['channels'])\n        data = cellarray(matfile['data'])\n    \n\n        ## New: Resample to 200 Hz!!! %\n        #Fs =int( Fs.item())\n        #P = 200\n        #Q = Fs\n        #print('resample',Q,P)\n        #data = scipy.signal.resample(data.T, int(len(data.T) * P / Q))\n        ### data = np.resample(data, 200, Fs)\n        #data = cellarray(data.T)\n        Fs = 200\n\n        def strrep_s(a, x, y):\n            return a.replace(x, y)\n\n        # =strcmp(labels,\"['f8']\")\n        strrep = np.vectorize(strrep_s)\n\n        print('channels', channels)\n        print(size(channels, 1))\n        # df_s0=pd.DataFrame()\n        labels = copy(channels)\n        # print(labels.shape)\n\n        for i in arange(1, size(channels, 1)).reshape(-1):\n            x = strrep(channels[i, :], ' ', '')\n            labels[i] = lower(x)\n\n        disp('load 0')\n        disp('compute 1','calculate locational differential voltage')\n        # pd=copy(pwd)\n        movingwin = matlabarray(cat(2, 2))\n\n        # dictobj\n        params = Dict2Obj({'default': 1})\n\n        params.pad = copy(0)\n        params.fpass = copy(cat(0.5, 20))\n        params.err = copy(0)\n        params.trialave = copy(0)\n        params.tapers = copy(cat(2, 3))\n        params.Fs = copy(200)\n\n        # spect=cell(4,4)\n        # COI=numpy.empty((4, ), dtype=object)\n        COI = cell(4, )\n        COI[1] = cellarray([['fp1', 'f7'], ['f7', 't3'], ['t3', 't5'], ['t5', 'o1']])\n        COI[2] = cellarray([['fp1', 'f3'], ['f3', 'c3'], ['c3', 'p3'], ['p3', 'o1']])\n        COI[3] = cellarray([['fp2', 'f4'], ['f4', 'c4'], ['c4', 'p4'], ['p4', 'o2']])\n        COI[4] = cellarray([['fp2', 'f8'], ['f8', 't4'], ['t4', 't6'], ['t6', 'o2']])\n\n        # test\n        def strcmp_s(x, y):\n            return x == y\n\n        strcmp = np.vectorize(strcmp_s)\n\n        dataB = cell(4, 4)\n\n        # print(data.shape,data.__class__,dataB.shape,dataB.__class__,data.dtype,dataB.dtype)\n        for kk in arange(1, 4).reshape(-1):\n            # print('kk',kk)\n            coi = COI[kk]\n            print('coi', coi, size(coi, 1), size(coi, 2), size(coi))\n\n            for k in arange(1, size(coi, 1)).reshape(-1):\n                print('kk', kk, 'k', k, coi[k, 1], coi[k, 2])\n                # print(coi[k, 1 - 1], coi[k, 2 - 1])\n                # print('k',k,coi,'labels')\n                # print(coi[k,1-1],coi[k,2-1])\n\n                c1 = find(strcmp(labels, coi[k, 1]))\n                c2 = find(strcmp(labels, coi[k, 2]))\n                c1 = c1.item()\n                c2 = c2.item()\n                # dataB[kk,k]=data(c1,arange()) - data(c2,arange())\n                # print('items',c1,c2)\n                dataB[kk, k] = np.array((data[c1, :] - data[c2, :]).tolist())    \n    \n\n        # Par version\n        n_jobs = -1  # number of cpus for parallel computing, -1 is all cpus\n        verbose = True  # verbosity in parallel computing\n        res = Parallel(n_jobs=n_jobs, verbose=verbose)(\n            delayed(mt_spectrogram)(dataB[kk,k], Fs) for kk,k in itertools.product([1,2,3,4],[\n                1,2,3,4])\n        )\n\n        \n        # Step 3\n        spect = cell(4, 4)\n        print(len(res))\n\n        for i, rr in enumerate(res):\n            # rr[0][freq_good_ids]\n            ii=i % 4\n            iii=floor(i/4)\n            spect[iii, ii + 1] = rr[0].T\n\n        stimes=res[0][1]\n        sfreqs = res[0][2]\n        \n        \n \n        R = cell(4, 1)\n        # R=np.empty((4,1),dtype=object)\n        for kk in arange(1, 4).reshape(-1):\n            # print('kk',kk,'size',size(spect[kk,1-1]),spect[kk,1-1].shape,tuple(size(spect[kk,1-1])))\n            T = np.zeros(size(spect[kk, 1]))\n            # print('T',T)\n            # T = zeros(sizespect[kk, 1 - 1].shape[0])\n            print('T', T.shape)\n            for k in arange(1, 4).reshape(-1):\n                # print('kk',kk,'k',k)\n\n                T = T + spect[kk, k]\n            R[kk] = T / 4\n\n\n\n        epoch_length = dot(Fs, 2)\n        window_length = dot(cat(14, 10, 6, 2), Fs)\n\n        # filtering of data\n        fnyq = Fs / 2  # nyquist frequency\n        fc_high = 50\n        fc_low = 0.5\n        #  cutoff frequencies\n        # b,a=butter(6,cat(fc_low / fnyq,fc_high / fnyq),'bandpass',nargout=2)\n        b, a = butter(6, cat(fc_low / fnyq, fc_high / fnyq), 'bandpass')\n\n        for kk in arange(1, 4).reshape(-1):\n            for k in arange(1, 4).reshape(-1):\n                # print('kk',kk,'k',k)\n                temp = dataB[kk, k]\n                # dataB[kk, k] = filtfilt(b, a, temp)\n                # dataB[kk,k]=filtfilt(b,a,temp,padtype='odd')\n\n                # dataB[kk, k] = lfilter(b, a, temp)\n                dataB[kk, k] = filtfilt(b, a, temp, padtype='odd', padlen=3 * (np.amax((len(a), len(b))) - 1))\n                \n        \n        # Step 5\n        spectwindow_length = floor(window_length / Fs / 2)\n\n        N = length(dataB[1, 1])\n        nr_epochs = floor((N - (window_length[1] - epoch_length) - Fs) / epoch_length)\n        nr_features = 144\n\n        features = cell(4, nr_features, nr_epochs)\n        for kk in arange(1, 4).reshape(-1):\n            \n            spect = R[kk, 1]\n            data = dataB[kk, :]\n\n            #Par version\n            n_jobs = -1 # number of cpus for parallel computing, -1 is all cpus\n            verbose = True  # verbosity in parallel computing\n            res = Parallel(n_jobs=n_jobs, verbose=verbose)(\n                # delayed(compute_spec_each_seg)(eeg[window_start[wi]:window_start[wi] + window_length, :], NW, Fs) for wi in range(window_num)\n                delayed(get_features)(data, int(i.item()), window_length, spect, sfreqs, spectwindow_length, Fs) for i\n                in arange(1, nr_epochs).reshape(-1)\n            )\n\n            print(len(res))\n            for i, rr in enumerate(res):\n                # rr[0][freq_good_ids]\n                features[kk, :, i + 1] = rr[:]\n\n        features = features.reshape(cat(dot(4, nr_features), length(features)))\n        features = features.T            \n            \n            \n        # Step 6\n        fileName0=elt\n        saveFeats = features.T\n\n        # turn this into a pandas df\n        saveFeats = pd.DataFrame(saveFeats)\n\n        # add time stamp\n        saveFeats.reset_index(inplace=True)\n        fileNum = fileName0[:len(fileName0) - 4]\n        import re\n        fileNum = re.search('(\\d+)$', fileNum).group(0)\n        sampleRate = round(Fs)\n\n        def timeStamp(ind, fileNum, sampleRate):\n            start = (int(fileNum) - 1) * 99999\n            row = start + int(ind)\n            time = row * int(sampleRate)\n            return time\n\n        saveFeats['time'] = saveFeats['index'].apply(lambda e: timeStamp(e, fileNum, sampleRate))\n\n        # add case id\n        caseNum = int(re.search(r'\\d+', fileName0).group())\n        saveFeats['case'] = caseNum\n\n        try:\n            print(fileName0)\n            onlyFiName = os.path.basename(fileName0)\n            fp = os.path.basename(fileName0) + '.csv'\n            print(fp)\n            savep = absdir + 'MGH-100P/new_features_' + fp\n            print('save to ', savep)\n            if not os.path.isfile(savep):\n                saveFeats.to_csv(savep)\n        \n        except:\n            print('[ERROR]', 'saving to csv')\n        \n        \n        \n        #Fs=matfile['Fs'].item()\n        #dataset=matfile['data']\n        #start_time=matfile['start_time']\n\n        #Parallelize spectrogram\n        #n_jobs=-1\n        #verbose=10\n        #res = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        #   delayed(mt_spectrogram)(cellarray(matfile['data'][i,:]), Fs) for i in range(1,16))\n        msg='No issues'\n    except Exception as e:\n        print('error')\n        #msg=str(e)\n        #print('error')\n        Fs=''\n        start_time=''\n        dataset=(0,0)\n        dataB=(0,0)\n        msg=str(e)        \n        #dataset=dataset.shape\n        \n        \n        ## Parallelize filtering\n        #out = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        #   delayed(filtering_data)(data[i],Fs) for i in range(16))\n\n    #print(dataB.shape)\n    return (elt, start_time, Fs, dataset, dataB,msg)\n\n    \nimport os\nimport hdf5storage\nimport numpy as np\nimport multiprocessing as mp\n\ndef testFunc(elt):\n    try:\n        absdir='/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/'\n        elt=absdir+elt\n        dataset=os.path.getsize(elt)\n        matfile = hdf5storage.loadmat(elt)\n        \n        cpu_count=mp.cpu_count()\n        \n      \n        Fs = int(np.round(matfile['Fs']))\n        start_time = matfile['start_time']\n        channels = matfile['channels']\n        data = matfile['data']\n        msg='No issue'\n    except Exception as e:\n        print('error')\n        msg=str(e)\n        \n    return (elt,start_time,data,data.shape,cpu_count,msg)\n\n# Benching 8\nrunt=True\nif (runt):\n    #!ls MGH-100P\n    \n    #print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\n    n_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\n    \n    #distData = sc.parallelize(lsub[:n_exec])\n    distData = sc.parallelize(lsub)\n    dtd=distData.repartition(min(n_exec,len(lsub)))\n    distData.cache()\n    dtd.cache()\n    \n    print('num part',dtd.getNumPartitions())\n    print('num data',len(lsub))\n    print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\n    print('data',lsub)\n    \n    testdtd=dtd.map(lambda e: mockFunc(e))\n    testdtd.cache()\n    %time lres=testdtd.take(len(lsub))    \n    print(dtd.collect(),lres,len(lres))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Segment Load\nimport os\nimport numpy as np\nimport hdf5storage\nimport spectrum\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom numpy.core.defchararray import lower\nfrom numpy import floor\nimport swiftclient\nimport pandas as pd\n#from compute_spectrogram_sunhaoqi import mtspecgram_shq\nfrom get_features import get_features\nfrom scipy.signal import butter, filtfilt, lfilter\nfrom runtime import cellarray, matlabarray, cat, cell, arange, size, disp, find, dot, copy, length, Dict2Obj,catstr\n\nfrom mtspec import mt_spectrogram\n\ndef load(fileName0):\n    #matfile = hdf5storage.loadmat(fp)\n    \n    #return matfile\n\n    absdir='/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/'\n\n    fileName0=absdir+fileName0\n    print(fileName0)\n    \n    #if not os.path.isfile(fileName0):\n    #    #print('writing to local',fileName0)    \n    #    container = 'MGH'\n    #    resp_headers, obj_contents = conn.get_object(container, fileName0)\n    #    #with open(os.path.basename(fileName0), 'wb') as local:\n    #    with open(fileName0, 'wb') as local:\n    #        local.write(obj_contents)\n\n    #print('load file',fileName0)\n    matfile = hdf5storage.loadmat(fileName0)\n    #matfile = hdf5storage.loadmat(os.path.basename(fileName0))\n\n    Fs=cellarray(int(np.round(matfile['Fs'])))\n    start_time = cellarray(matfile['start_time'])\n    channels=cellarray(matfile['channels'])\n    data = cellarray(matfile['data'])\n    \n\n    # New: Resample to 200 Hz!!! %    \n    Fs=Fs.item()\n    print('Fs',Fs)\n    # data=scipy.signal.resample(data.T,200,200)\n    P=200\n    Q=int(Fs)\n    data=scipy.signal.resample(data.T, int(len(data.T) * P / Q))\n    ## data = np.resample(data, 200, Fs)\n    data = cellarray(data.T)\n    Fs = 200\n\n\n    def strrep_s(a,x,y):\n        return a.replace(x,y)\n\n    #=strcmp(labels,\"['f8']\")\n    strrep=np.vectorize(strrep_s)\n\n    print('channels',channels)\n    print(size(channels,1))\n    # df_s0=pd.DataFrame()\n    labels=copy(channels)\n    print(labels.shape)\n    \n    for i in arange(1, size(channels, 1)).reshape(-1):\n        x=strrep(channels[i,:],' ','')\n        labels[i] = lower(x)\n\n    disp('load 0')\n    disp('compute 1')\n    # pd=copy(pwd)\n    movingwin=matlabarray(cat(2,2))\n\n    # dictobj\n    params=Dict2Obj({'default':1})\n\n    params.pad = copy(0)\n    params.fpass = copy(cat(0.5,20))\n    params.err = copy(0)\n    params.trialave = copy(0)\n    params.tapers = copy(cat(2,3))\n    params.Fs = copy(200)\n\n    # spect=cell(4,4)\n    # COI=numpy.empty((4, ), dtype=object)\n    COI=cell(4,)\n    COI[1]=cellarray([['fp1','f7'],['f7','t3'],['t3','t5'],['t5','o1']])\n    COI[2]=cellarray([['fp1','f3'],['f3','c3'],['c3','p3'],['p3','o1']])\n    COI[3]=cellarray([['fp2','f4'],['f4','c4'],['c4','p4'],['p4','o2']])\n    COI[4]=cellarray([['fp2','f8'],['f8','t4'],['t4','t6'],['t6','o2']])\n\n    print('COI',COI)\n    \n    ROInickname=cellarray(['LL','LP','RP','RL'])\n    print(ROInickname)\n\n    # test\n    def strcmp_s(x,y):\n        return x == y\n\n    \n    strcmp=np.vectorize(strcmp_s)\n    #=strcmp(labels,\"['f8']\")\n    # print(strcmp(ROInickname,'LL'))\n\n    dataB=cell(4,4)\n    \n    # print(data.shape,data.__class__,dataB.shape,dataB.__class__,data.dtype,dataB.dtype)\n    for kk in arange(1 , 4 ).reshape(-1):\n        # print('kk',kk)\n        coi=COI[kk]\n        print('coi',coi,size(coi,1),size(coi,2),size(coi))\n\n        for k in arange(1 , size(coi, 1) ).reshape(-1):\n            print('kk',kk,'k',k , coi[k,1],coi[k,2])\n            # print(coi[k, 1 - 1], coi[k, 2 - 1])\n            # print('k',k,coi,'labels')\n            # print(coi[k,1-1],coi[k,2-1])\n\n            c1=find(strcmp(labels,coi[k,1]))\n            c2=find(strcmp(labels,coi[k,2]))\n            c1=c1.item()\n            c2 = c2.item()\n            #dataB[kk,k]=data(c1,arange()) - data(c2,arange())\n            print('items',c1,c2)\n            dataB[kk, k] = np.array((data[c1, :] - data[c2, :]).tolist())\n\n    #d={}\n    #d['dataB']=dataB\n    return dataB\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "2\n"
                }
            ], 
            "source": "from itertools import product\nimport glob\n# Create a (case, channel, window) rdd\n#def optim_load_spectrogram(fileName0):\ndef loadData_seg(lst):\n    for e in lst:\n        param=e.split('_')\n    #hdf5storage\n    \n    return l\n\ndef optim_load_spectrogram(dir0):    \n    #mat=fileName0\n    \n    #fn=os.path.basename(fileName0)\n    \n    \n    \n    win_size=100\n    absdir='/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work'\n    rdd0=[dir0+'/'+os.path.basename(elt) for elt in glob.glob(absdir+'/'+dir0+'/*.mat')]\n    \n    rdd01=[]\n    for elt in rdd0:\n        #print(elt)\n        dataB=load(elt)\n        #dataB='elt'\n    \n        rdd1=[str(k1)+'_'+str(k2) for k1,k2 in product( [1,2,3,4], [1,2,3,4] )]\n        \n        rdd01.append([(str(e1)+'_'+str(e2),dataB[e2[0],e2[1]]) for e1,e2 in product(rdd0,rdd1)])\n        #rdd01=[(str(e1)+'_'+str(e2),dataB[e2[0],e2[1]]) for e1,e2 in product(rdd0,rdd1)]\n        \n        #rdd01.append([(str(e1)+'_'+str(e2)) for e1,e2 in product(rdd0,rdd1)])\n    \n    #winsize\n    #rdd012=[elt+'_'+str(i) for elt,i in product(rdd01,range(1,win_size))]\n    \n    #rdddata=loadData_seg(rdd01)\n    \n    # load_data\n    return rdd01\n\n#tsData=['Case6/Case6_seg1.mat','Case6/Case6_seg2.mat']\ntsData=['Case5','Case6','Case4']\nrdd=sc.parallelize(tsData)\nrdd.repartition(len(tsData))\nprint(rdd.getNumPartitions())\n\nl=rdd.flatMap(lambda e: optim_load_spectrogram(e))\n\n%time combined=l.collect()\nprint(len(combined))\nprint(combined)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# FEATS\nln=!ls -l --time-style=+\"|%Y-%m-%d_%H:%M:%S|\" MGH-100P/*.csv | awk '{print $5,$6,$7}'\nl=[e.split(' ') for e in ln]\ndf_feats=pd.DataFrame(l,columns=['feat_size','feat_create_time','feat_filename'])\ndf_feats['token']=df_feats.feat_filename.str.extract('.*(Case.*)\\.csv',expand=True)\n\n\n#print(l)\n# MAT\n#ln=!ls -l Case*/*.mat | awk '{print $5,$9}' | grep 'mat'\n##print(ln)\n#l=[e.split(' ') for e in ln]\n#df_mats=pd.DataFrame(l,columns=['size','filename'])\n## scale to pyspark\n#df_mats['token']=df_mats.filename.str.extract('.*(Case.*)',expand=True)"
        }, 
        {
            "execution_count": 138, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Test broadcast\n#Fs=200\n#sc.broadcast(Fs)\nimport h5py\nimport numpy as np\nchr_s=np.vectorize(lambda e: chr(e))\n\ndef load_h5(elt):\n#elt='Case5/Case5_seg1.mat'\n    absdir='/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/'\n    elt=absdir+elt\n    \n    os.system('touch /gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/pid/%s ' % elt)\n    try:\n        with h5py.File(elt, 'r') as fp:\n            l=fp['start_time'][:]\n            #l=dataset\n            l=[chr(e) for e in l]\n            s=''.join(l)\n            start_time=s\n            print(start_time)\n\n            l=fp['Fs'][:]\n            Fs=l.item()\n            print(Fs)\n\n            l=fp['channels'][:].T\n            l=chr_s(l)\n            channels=[''.join(elt) for elt in l]\n            #print(l)\n\n            #l=fp['data'][:]\n            #data=l.T\n            #print(data.shape)\n    except:\n        start_time,data,channels,Fs='','','',''\n        \n    return (start_time,data,channels,Fs)\n\n\n"
        }, 
        {
            "execution_count": 140, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['578104054 Case86/Case86_seg1.mat', '1096842818 Case86/Case86_seg2.mat', '1161036307 Case86/Case86_seg3.mat', '656064928 Case86/Case86_seg4.mat', '1114130078 Case87/Case87_seg1.mat', '1134503145 Case87/Case87_seg2.mat', '860641912 Case87/Case87_seg3.mat', '1124481622 Case87/Case87_seg4.mat', '1151680241 Case88/Case88_seg1.mat', '331460354 Case88/Case88_seg2.mat', '155046257 Case88/Case88_seg3.mat', '944810229 Case88/Case88_seg4.mat', '1088126215 Case88/Case88_seg5.mat', '449743871 Case88/Case88_seg6.mat', '69792334 Case88/Case88_seg7.mat', '972731098 Case89/Case89_seg1.mat', '673094788 Case89/Case89_seg2.mat', '31401940 Case89/Case89_seg3.mat', '711770823 Case89/Case89_seg4.mat', '735714479 Case89/Case89_seg5.mat', '661416598 Case89/Case89_seg6.mat', '584447357 Case89/Case89_seg7.mat', '1080331034 Case8/Case8_seg1.mat', '759021302 Case8/Case8_seg2.mat', '1112372751 Case8/Case8_seg3.mat', '912344960 Case8/Case8_seg4.mat', '260749860 Case8/Case8_seg5.mat', '35786830 Case8/Case8_seg6.mat', '120718134 Case8/Case8_seg7.mat']\n         size                filename            token\n0   578104054  Case86/Case86_seg1.mat  Case86_seg1.mat\n1  1096842818  Case86/Case86_seg2.mat  Case86_seg2.mat\n2  1161036307  Case86/Case86_seg3.mat  Case86_seg3.mat\n3   656064928  Case86/Case86_seg4.mat  Case86_seg4.mat\n4  1114130078  Case87/Case87_seg1.mat  Case87_seg1.mat\n10\nnum exec 9\n['Case86/Case86_seg1.mat', 'Case86/Case86_seg2.mat', 'Case86/Case86_seg3.mat', 'Case86/Case86_seg4.mat', 'Case87/Case87_seg1.mat', 'Case87/Case87_seg2.mat', 'Case87/Case87_seg3.mat', 'Case87/Case87_seg4.mat', 'Case88/Case88_seg1.mat', 'Case88/Case88_seg2.mat']\n"
                }, 
                {
                    "ename": "Py4JJavaError", 
                    "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 10 times, most recent failure: Lost task 1.9 in stage 18.0 (TID 374, yp-spark-dal09-env5-0019, executor c4ee6b02-8d4e-4049-b0da-bff8c2793302): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-c014b8d4ad0d>\", line 30, in <lambda>\n  File \"<ipython-input-138-72e8c8d75992>\", line 11, in load_h5\nTypeError: Can't convert 'list' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:954)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:953)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-c014b8d4ad0d>\", line 30, in <lambda>\n  File \"<ipython-input-138-72e8c8d75992>\", line 11, in load_h5\nTypeError: Can't convert 'list' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-140-c014b8d4ad0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mresRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mdataRdd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mload_h5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 10 times, most recent failure: Lost task 1.9 in stage 18.0 (TID 374, yp-spark-dal09-env5-0019, executor c4ee6b02-8d4e-4049-b0da-bff8c2793302): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-c014b8d4ad0d>\", line 30, in <lambda>\n  File \"<ipython-input-138-72e8c8d75992>\", line 11, in load_h5\nTypeError: Can't convert 'list' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:954)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:953)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark21master/spark-2.1.0-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-c014b8d4ad0d>\", line 30, in <lambda>\n  File \"<ipython-input-138-72e8c8d75992>\", line 11, in load_h5\nTypeError: Can't convert 'list' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "ln=!ls -l Case*/*.mat | awk '{print $5,$9}' | grep 'mat'\nprint(ln)\nl=[e.split(' ') for e in ln]\ndf_mats=pd.DataFrame(l,columns=['size','filename'])\n# scale to pyspark\ndf_mats['token']=df_mats.filename.str.extract('.*(Case.*)',expand=True)\n\ndf_mats=df_mats[0:10]\nprint(df_mats.head())\nprint(len(df_mats))\n\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import functions as F\n\n\nprint('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\nn_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\n\nl_data=list(df_mats.ix[:10,'filename'])\nsqlctx=SQLContext(sc)\nrdd=sc.parallelize(l)\n\n# Dataframe\n#rdd=rdd.map(lambda e: e.split(' '))\n#s=rdd.collect()\n#sdf=sqlctx.createDataFrame(s).show()\nprint(l_data)\nresRdd=rdd.repartition(10)\nresRdd.cache()\ndataRdd=resRdd.map(lambda e: load_h5(e))\ndata=dataRdd.collect()\n\nprint(data)\n\n#sqlCtx = SQLContext(sc)\n#sqlCtx.createDataFrame(df).show()\n\n#print(sdf)\n"
        }, 
        {
            "execution_count": 81, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "       src_size        src_create_time               src_filename  \\\n0      41984256  |2017-07-04_22:11:17|   Case101/Case101_seg1.mat   \n1     854189583  |2017-07-04_22:11:36|   Case101/Case101_seg2.mat   \n2     885963922  |2017-07-04_22:13:04|   Case101/Case101_seg6.mat   \n3     868371141  |2017-07-04_22:13:20|   Case102/Case102_seg1.mat   \n4    1223459230  |2017-07-04_22:14:11|   Case102/Case102_seg4.mat   \n5      38349277  |2017-07-04_22:14:24|   Case102/Case102_seg6.mat   \n6      26548279  |2017-07-04_22:15:50|   Case103/Case103_seg6.mat   \n7      41487984  |2017-07-04_23:02:53|   Case103/Case103_seg9.mat   \n8    1565238074  |2017-07-04_23:03:43|   Case104/Case104_seg2.mat   \n9    1339184685  |2017-07-04_23:05:15|   Case105/Case105_seg2.mat   \n10     28870262  |2017-07-04_23:05:38|   Case105/Case105_seg5.mat   \n11    780845365  |2017-07-04_23:07:05|   Case106/Case106_seg3.mat   \n12     44725884  |2017-07-04_23:07:08|   Case107/Case107_seg1.mat   \n13    943603415  |2017-07-04_23:07:27|   Case107/Case107_seg2.mat   \n14    365848108  |2017-07-04_23:11:47|  Case108/Case108_seg10.mat   \n15     26351165  |2017-07-04_23:12:35|  Case108/Case108_seg15.mat   \n16    133409215  |2017-07-04_23:10:47|   Case108/Case108_seg5.mat   \n17     27939218  |2017-07-04_23:12:36|   Case109/Case109_seg1.mat   \n18   1050627499  |2017-07-04_23:12:56|   Case109/Case109_seg2.mat   \n19    719514211  |2017-07-04_23:14:13|   Case109/Case109_seg6.mat   \n20     35631918  |2017-07-04_19:06:10|     Case10/Case10_seg3.mat   \n21     66394197  |2017-07-04_19:06:34|     Case10/Case10_seg5.mat   \n22    911647737  |2017-07-04_19:07:21|     Case10/Case10_seg7.mat   \n23     34060112  |2017-07-04_23:14:15|   Case110/Case110_seg1.mat   \n24     85151126  |2017-07-04_23:15:51|   Case111/Case111_seg1.mat   \n25     35158021  |2017-07-04_23:15:52|   Case111/Case111_seg2.mat   \n26    102857900  |2017-07-04_23:16:33|   Case111/Case111_seg5.mat   \n27     26923796  |2017-07-04_23:18:47|   Case112/Case112_seg5.mat   \n28   1068644374  |2017-07-04_23:19:23|   Case112/Case112_seg6.mat   \n29     22456296  |2017-07-05_00:29:49|   Case118/Case118_seg4.mat   \n..          ...                    ...                        ...   \n73   1111932176  |2017-07-04_20:41:06|     Case51/Case51_seg6.mat   \n74     68352354  |2017-07-04_20:41:10|     Case52/Case52_seg1.mat   \n75     37459843  |2017-07-04_20:42:27|     Case52/Case52_seg4.mat   \n76    521014732  |2017-07-04_20:45:19|     Case53/Case53_seg3.mat   \n77     50843130  |2017-07-04_20:45:20|     Case53/Case53_seg4.mat   \n78    229958578  |2017-07-04_20:46:24|     Case53/Case53_seg7.mat   \n79    835507412  |2017-07-04_20:47:18|     Case53/Case53_seg9.mat   \n80     37985746  |2017-07-04_20:47:57|     Case54/Case54_seg3.mat   \n81   1221252180  |2017-07-04_20:49:07|     Case55/Case55_seg2.mat   \n82     50467105  |2017-07-04_20:51:13|     Case56/Case56_seg1.mat   \n83    761965935  |2017-07-04_20:51:34|     Case56/Case56_seg2.mat   \n84    841675900  |2017-07-04_20:51:48|     Case56/Case56_seg3.mat   \n85    843628268  |2017-07-04_20:52:00|     Case56/Case56_seg4.mat   \n86    237839555  |2017-07-04_20:53:05|     Case57/Case57_seg3.mat   \n87     41705170  |2017-07-04_20:53:48|     Case57/Case57_seg6.mat   \n88     55984922  |2017-07-04_20:54:06|     Case58/Case58_seg1.mat   \n89    455308303  |2017-07-04_20:54:31|     Case58/Case58_seg3.mat   \n90    208403960  |2017-07-04_20:54:41|     Case59/Case59_seg1.mat   \n91    619967981  |2017-07-04_20:55:04|     Case59/Case59_seg2.mat   \n92     54083139  |2017-07-04_20:55:05|     Case59/Case59_seg3.mat   \n93    162191371  |2017-07-04_19:00:01|       Case5/Case5_seg5.mat   \n94     44696329  |2017-07-04_19:00:02|       Case5/Case5_seg6.mat   \n95     60234523  |2017-07-04_20:55:08|     Case60/Case60_seg1.mat   \n96    148491517  |2017-07-04_21:09:24|     Case72/Case72_seg2.mat   \n97     99464982  |2017-07-04_21:09:59|     Case73/Case73_seg4.mat   \n98     70020593  |2017-07-04_21:12:15|     Case75/Case75_seg8.mat   \n99     55773040  |2017-07-04_21:20:06|    Case78/Case78_seg10.mat   \n100    71627847  |2017-07-04_21:19:10|     Case78/Case78_seg6.mat   \n101    31401940  |2017-07-04_22:05:56|     Case89/Case89_seg3.mat   \n102    35786830  |2017-07-04_19:02:47|       Case8/Case8_seg6.mat   \n\n                 token feat_size       feat_create_time  \\\n0     Case101_seg1.mat   3292379  |2017-07-14_16:40:55|   \n1     Case101_seg2.mat  89217736  |2017-07-15_05:13:37|   \n2     Case101_seg6.mat  65818365  |2017-07-15_02:25:50|   \n3     Case102_seg1.mat  55344424  |2017-07-15_04:46:53|   \n4     Case102_seg4.mat  88298971  |2017-07-15_05:26:20|   \n5     Case102_seg6.mat  15093574  |2017-07-19_08:52:01|   \n6     Case103_seg6.mat   6568452  |2017-07-14_23:07:56|   \n7     Case103_seg9.mat  14102608  |2017-07-18_23:02:36|   \n8     Case104_seg2.mat  86221832  |2017-07-15_04:21:03|   \n9     Case105_seg2.mat  88370500  |2017-07-15_05:27:33|   \n10    Case105_seg5.mat  17694024  |2017-07-18_22:28:54|   \n11    Case106_seg3.mat  67264446  |2017-07-15_02:28:26|   \n12    Case107_seg1.mat  19154956  |2017-07-19_03:13:59|   \n13    Case107_seg2.mat  89093272  |2017-07-15_05:23:58|   \n14   Case108_seg10.mat  29085441  |2017-07-14_23:41:08|   \n15   Case108_seg15.mat  16251307  |2017-07-18_22:15:16|   \n16    Case108_seg5.mat  53876583  |2017-07-20_11:53:03|   \n17    Case109_seg1.mat  14110273  |2017-07-17_22:40:34|   \n18    Case109_seg2.mat  88668773  |2017-07-15_06:25:27|   \n19    Case109_seg6.mat  60617136  |2017-07-15_07:42:46|   \n20     Case10_seg3.mat   8194933  |2017-07-18_22:23:50|   \n21     Case10_seg5.mat  17630676  |2017-07-20_13:04:03|   \n22     Case10_seg7.mat  59661525  |2017-07-15_03:10:04|   \n23    Case110_seg1.mat  11032012  |2017-07-18_21:42:27|   \n24    Case111_seg1.mat   9013572  |2017-07-14_23:06:46|   \n25    Case111_seg2.mat   7553687  |2017-07-14_23:14:18|   \n26    Case111_seg5.mat  23730287  |2017-07-20_13:22:17|   \n27    Case112_seg5.mat  15278487  |2017-07-19_03:10:24|   \n28    Case112_seg6.mat  88925907  |2017-07-15_05:25:29|   \n29    Case118_seg4.mat   5178087  |2017-07-18_21:00:07|   \n..                 ...       ...                    ...   \n73     Case51_seg6.mat  79275412  |2017-07-15_18:13:51|   \n74     Case52_seg1.mat   5434129  |2017-07-15_16:26:48|   \n75     Case52_seg4.mat   5416853  |2017-07-15_13:46:36|   \n76     Case53_seg3.mat  22301977  |2017-07-15_17:20:37|   \n77     Case53_seg4.mat   6078039  |2017-07-15_15:25:25|   \n78     Case53_seg7.mat  11134068  |2017-07-15_13:53:39|   \n79     Case53_seg9.mat  49443901  |2017-07-15_17:24:49|   \n80     Case54_seg3.mat   4111083  |2017-07-15_14:20:44|   \n81     Case55_seg2.mat  61615911  |2017-07-15_16:58:33|   \n82     Case56_seg1.mat   3125429  |2017-07-15_13:44:17|   \n83     Case56_seg2.mat  34603400  |2017-07-15_14:47:21|   \n84     Case56_seg3.mat  34582570  |2017-07-15_15:52:48|   \n85     Case56_seg4.mat  34627424  |2017-07-15_14:46:59|   \n86     Case57_seg3.mat  12476644  |2017-07-15_14:03:14|   \n87     Case57_seg6.mat   5870695  |2017-07-15_13:54:03|   \n88     Case58_seg1.mat   5700313  |2017-07-15_14:00:23|   \n89     Case58_seg3.mat  38486742  |2017-07-15_14:55:04|   \n90     Case59_seg1.mat  11470794  |2017-07-15_13:51:40|   \n91     Case59_seg2.mat  35450806  |2017-07-15_14:42:33|   \n92     Case59_seg3.mat   6679946  |2017-07-15_13:47:39|   \n93      Case5_seg5.mat  13133857  |2017-07-15_13:53:37|   \n94      Case5_seg6.mat   5388703  |2017-07-13_17:47:43|   \n95     Case60_seg1.mat  21534794  |2017-07-20_13:24:25|   \n96     Case72_seg2.mat  61823653  |2017-07-20_13:32:37|   \n97     Case73_seg4.mat  30763614  |2017-07-20_13:13:10|   \n98     Case75_seg8.mat  36462801  |2017-07-20_13:15:10|   \n99    Case78_seg10.mat  28334115  |2017-07-20_13:30:45|   \n100    Case78_seg6.mat  12990176  |2017-07-20_13:02:30|   \n101    Case89_seg3.mat  24700948  |2017-07-18_14:34:01|   \n102     Case8_seg6.mat  15083387  |2017-07-18_15:51:40|   \n\n                                   feat_filename            token:1  ratio  \n0     MGH-100P/new_features_Case101_seg1.mat.csv   Case101_seg1.mat     12  \n1     MGH-100P/new_features_Case101_seg2.mat.csv   Case101_seg2.mat      9  \n2     MGH-100P/new_features_Case101_seg6.mat.csv   Case101_seg6.mat     13  \n3     MGH-100P/new_features_Case102_seg1.mat.csv   Case102_seg1.mat     15  \n4     MGH-100P/new_features_Case102_seg4.mat.csv   Case102_seg4.mat     13  \n5     MGH-100P/new_features_Case102_seg6.mat.csv   Case102_seg6.mat      2  \n6     MGH-100P/new_features_Case103_seg6.mat.csv   Case103_seg6.mat      4  \n7     MGH-100P/new_features_Case103_seg9.mat.csv   Case103_seg9.mat      2  \n8     MGH-100P/new_features_Case104_seg2.mat.csv   Case104_seg2.mat     18  \n9     MGH-100P/new_features_Case105_seg2.mat.csv   Case105_seg2.mat     15  \n10    MGH-100P/new_features_Case105_seg5.mat.csv   Case105_seg5.mat      1  \n11    MGH-100P/new_features_Case106_seg3.mat.csv   Case106_seg3.mat     11  \n12    MGH-100P/new_features_Case107_seg1.mat.csv   Case107_seg1.mat      2  \n13    MGH-100P/new_features_Case107_seg2.mat.csv   Case107_seg2.mat     10  \n14   MGH-100P/new_features_Case108_seg10.mat.csv  Case108_seg10.mat     12  \n15   MGH-100P/new_features_Case108_seg15.mat.csv  Case108_seg15.mat      1  \n16    MGH-100P/new_features_Case108_seg5.mat.csv   Case108_seg5.mat      2  \n17    MGH-100P/new_features_Case109_seg1.mat.csv   Case109_seg1.mat      1  \n18    MGH-100P/new_features_Case109_seg2.mat.csv   Case109_seg2.mat     11  \n19    MGH-100P/new_features_Case109_seg6.mat.csv   Case109_seg6.mat     11  \n20     MGH-100P/new_features_Case10_seg3.mat.csv    Case10_seg3.mat      4  \n21     MGH-100P/new_features_Case10_seg5.mat.csv    Case10_seg5.mat      3  \n22     MGH-100P/new_features_Case10_seg7.mat.csv    Case10_seg7.mat     15  \n23    MGH-100P/new_features_Case110_seg1.mat.csv   Case110_seg1.mat      3  \n24    MGH-100P/new_features_Case111_seg1.mat.csv   Case111_seg1.mat      9  \n25    MGH-100P/new_features_Case111_seg2.mat.csv   Case111_seg2.mat      4  \n26    MGH-100P/new_features_Case111_seg5.mat.csv   Case111_seg5.mat      4  \n27    MGH-100P/new_features_Case112_seg5.mat.csv   Case112_seg5.mat      1  \n28    MGH-100P/new_features_Case112_seg6.mat.csv   Case112_seg6.mat     12  \n29    MGH-100P/new_features_Case118_seg4.mat.csv   Case118_seg4.mat      4  \n..                                           ...                ...    ...  \n73     MGH-100P/new_features_Case51_seg6.mat.csv    Case51_seg6.mat     14  \n74     MGH-100P/new_features_Case52_seg1.mat.csv    Case52_seg1.mat     12  \n75     MGH-100P/new_features_Case52_seg4.mat.csv    Case52_seg4.mat      6  \n76     MGH-100P/new_features_Case53_seg3.mat.csv    Case53_seg3.mat     23  \n77     MGH-100P/new_features_Case53_seg4.mat.csv    Case53_seg4.mat      8  \n78     MGH-100P/new_features_Case53_seg7.mat.csv    Case53_seg7.mat     20  \n79     MGH-100P/new_features_Case53_seg9.mat.csv    Case53_seg9.mat     16  \n80     MGH-100P/new_features_Case54_seg3.mat.csv    Case54_seg3.mat      9  \n81     MGH-100P/new_features_Case55_seg2.mat.csv    Case55_seg2.mat     19  \n82     MGH-100P/new_features_Case56_seg1.mat.csv    Case56_seg1.mat     16  \n83     MGH-100P/new_features_Case56_seg2.mat.csv    Case56_seg2.mat     22  \n84     MGH-100P/new_features_Case56_seg3.mat.csv    Case56_seg3.mat     24  \n85     MGH-100P/new_features_Case56_seg4.mat.csv    Case56_seg4.mat     24  \n86     MGH-100P/new_features_Case57_seg3.mat.csv    Case57_seg3.mat     19  \n87     MGH-100P/new_features_Case57_seg6.mat.csv    Case57_seg6.mat      7  \n88     MGH-100P/new_features_Case58_seg1.mat.csv    Case58_seg1.mat      9  \n89     MGH-100P/new_features_Case58_seg3.mat.csv    Case58_seg3.mat     11  \n90     MGH-100P/new_features_Case59_seg1.mat.csv    Case59_seg1.mat     18  \n91     MGH-100P/new_features_Case59_seg2.mat.csv    Case59_seg2.mat     17  \n92     MGH-100P/new_features_Case59_seg3.mat.csv    Case59_seg3.mat      8  \n93      MGH-100P/new_features_Case5_seg5.mat.csv     Case5_seg5.mat     12  \n94      MGH-100P/new_features_Case5_seg6.mat.csv     Case5_seg6.mat      8  \n95     MGH-100P/new_features_Case60_seg1.mat.csv    Case60_seg1.mat      2  \n96     MGH-100P/new_features_Case72_seg2.mat.csv    Case72_seg2.mat      2  \n97     MGH-100P/new_features_Case73_seg4.mat.csv    Case73_seg4.mat      3  \n98     MGH-100P/new_features_Case75_seg8.mat.csv    Case75_seg8.mat      1  \n99    MGH-100P/new_features_Case78_seg10.mat.csv   Case78_seg10.mat      1  \n100    MGH-100P/new_features_Case78_seg6.mat.csv    Case78_seg6.mat      5  \n101    MGH-100P/new_features_Case89_seg3.mat.csv    Case89_seg3.mat      1  \n102     MGH-100P/new_features_Case8_seg6.mat.csv     Case8_seg6.mat      2  \n\n[103 rows x 9 columns]\n"
                }
            ], 
            "source": "import pandas as pd\nfrom pandasql import sqldf\n# pandasql is sqlite syntax\n\nln=!ls -l --time-style=+\"|%Y-%m-%d_%H:%M:%S|\" Case*/*.mat | awk '{print $5,$6,$7}'\n\n#l=ln.split('\\n')\n#print(ln)\nl=[e.split(' ') for e in ln]\ndfl=pd.DataFrame(l,columns=['src_size','src_create_time','src_filename'])\ndfl['token']=dfl.src_filename.str.extract('.*/(Case.*)',expand=True)\ndfl.head()\n\n\ndfout=sqldf(\"select * from (select *, cast(i0.src_size as decimal)/cast(r0.feat_size as decimal) as ratio from dfl i0 left join df_feats r0 on i0.token=r0.token) where ratio > 0\",globals())\nprint(dfout)\n#from pyspark import SQLContext\n#sqlctx=SQLContext(sc)\n#sdf = sqlctx.createDataFrame(dfout)\n#print(sdf.printSchema())\n##sqldf(\"select * from dfl i0 order by cast(size as integer) left join df_feats r0 on i0.token=r0.token\")"
        }, 
        {
            "execution_count": 77, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "driver\n1\n"
                }
            ], 
            "source": "from __future__ import print_function\n#[elt for elt in sc._conf.getAll()]\n\nd=dict(sc._conf.getAll())\nprint(d['spark.executor.id'])\n\n#sc.getConf.getAll.foreach(print)\nprint(sc._jsc.sc().getExecutorMemoryStatus().size())"
        }, 
        {
            "execution_count": 65, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# windows speed up\n\n!mkdir MGH-100P-SPECTRO"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# %load get_features.py\n\nimport hdf5storage\nfrom matplotlib.mlab import prctile\nfrom numpy import dot\nfrom numpy import mean, std, ceil\nfrom scipy.stats import kurtosis\n\nfrom fcn_shannon_entro import fcn_shannon_entro\n\n# get_features.m\nimport numpy as np\nimport scipy\n\n\ndef bandpower(Pxx, f, frange, opt='psd'):\n    \"\"\" integrate the power spectral density between fmin and fmax\n        using the trapezoidal method\n    \"\"\"\n    (fmin, fmax) = frange\n    ind_min = scipy.argmax(f > fmin) - 1\n    ind_max = scipy.argmax(f > fmax) + 1\n    # print(ind_min,ind_max)\n    return Pxx[ind_min: ind_max].sum(axis=0) * np.diff(f)[0]\n\n\nfrom runtime import cellarray, matlabarray, cat, length, arange, zeros, disp, floor, cell, end\n\n\ndef get_features(data=None, i=None, window_length=None, spect=None, sfreqs=None, spectwindow_length=None, Fs=None,\n                 *args, **kwargs):\n    # time features: line length, kurtosis, shannon entropy and nonlinear energy.\n    # spectral features: delta, theta, alpha and beta bandpower. delta to theta\n    # ratio, theta to alpha ratio and delta to alpha ratio. Kurtosis of delta,\n    # theta, alpha and beta bandpower.\n    if (i%100) == 0:\n        print(\"compute\", i,'fast')\n\n    line1avg = 0\n    line2avg = 0\n    line3avg = 0\n    line4avg = 0\n\n    kurt1avg = 0\n    kurt2avg = 0\n    kurt3avg = 0\n    kurt4avg = 0\n\n    shan1avg = 0\n    shan2avg = 0\n    shan3avg = 0\n    shan4avg = 0\n\n    psy1meanavg = 0\n    psy2meanavg = 0\n    psy3meanavg = 0\n    psy4meanavg = 0\n\n    psy1stdavg = 0\n    psy2stdavg = 0\n    psy3stdavg = 0\n    psy4stdavg = 0\n\n\n    # window_length = [14 10 6 2]*Fs; # the different windows for each segment\n    t = dot((i - 1), window_length[4]) + dot(0.5, window_length[1])\n    for k in arange(1, 4).reshape(-1):\n        # win1=data[k][arange(t - dot(0.5,window_length[1]) + 1,t + dot(0.5,window_length[1]))]\n        # win2=data[k][arange(t - dot(0.5,window_length[2]) + 1,t + dot(0.5,window_length[2]))]\n        # win3=data[k][arange(t - dot(0.5,window_length[3]) + 1,t + dot(0.5,window_length[3]))]\n        # win4=data[k][arange(t - dot(0.5,window_length[4]) + 1,t + dot(0.5,window_length[4]))]\n        \n        win1o = data[k][t - dot(0.5, window_length[1]):t + dot(0.5, window_length[1])]\n        win2o = data[k][t - dot(0.5, window_length[2]):t + dot(0.5, window_length[2])]\n        win3o = data[k][t - dot(0.5, window_length[3]):t + dot(0.5, window_length[3])]\n        win4o = data[k][t - dot(0.5, window_length[4]):t + dot(0.5, window_length[4])]\n\n        win1 = matlabarray(data[k][t - dot(0.5, window_length[1]):t + dot(0.5, window_length[1])])\n        win2 = matlabarray(data[k][t - dot(0.5, window_length[2]):t + dot(0.5, window_length[2])])\n        win3 = matlabarray(data[k][t - dot(0.5, window_length[3]):t + dot(0.5, window_length[3])])\n        win4 = matlabarray(data[k][t - dot(0.5, window_length[4]):t + dot(0.5, window_length[4])])\n\n        # time features\n        #for j in arange(1, window_length[1] - 1).reshape(-1):\n            # line1 = line1 + abs(win1[J + 1] - win1[j])\n        J=arange(1, window_length[1] - 1).reshape(-1)\n        line1 = np.sum(np.abs(win1[J + 1] - win1[J])) / (window_length[1] - 1)\n\n        # for j in arange(1, window_length[2] - 1).reshape(-1):\n        #     line2 = line2 + abs(win2[j + 1] - win2[j])\n        J=arange(1, window_length[2] - 1).reshape(-1)\n        line2 = np.sum(np.abs(win2[J + 1] - win2[J])) / (window_length[2] - 1)\n\n        # for j in arange(1, window_length[3] - 1).reshape(-1):\n        #     line3 = line3 + abs(win3[j + 1] - win3[j])\n        J=arange(1, window_length[3] - 1).reshape(-1)\n        line3 = np.sum(np.abs(win3[J + 1] - win3[J])) / (window_length[3] - 1)\n\n        # for j in arange(1, window_length[4] - 1).reshape(-1):\n        #     line4 = line4 + abs(win4[j + 1] - win4[j])\n        J=arange(1, window_length[4] - 1).reshape(-1)\n        line4 = np.sum(np.abs(win4[J + 1] - win4[J])) / (window_length[4] - 1)\n\n        kurt1 = kurtosis(win1o, 0)\n        kurt2 = kurtosis(win2o, 0)\n        kurt3 = kurtosis(win3o, 0)\n        kurt4 = kurtosis(win4o, 0)\n\n        #     samp_en1 = fcn_SampEn(2, 0.2*std(win1), win1); # sample entropy\n        #     samp_en2 = fcn_SampEn(2, 0.2*std(win2), win2);\n        #     samp_en3 = fcn_SampEn(2, 0.2*std(win3), win3);\n        #     samp_en4 = fcn_SampEn(2, 0.2*std(win4), win4);\n        shan1 = abs(fcn_shannon_entro(win1.T))\n        shan2 = abs(fcn_shannon_entro(win2.T))\n        shan3 = abs(fcn_shannon_entro(win3.T))\n        shan4 = abs(fcn_shannon_entro(win4.T))\n\n\n        psy1 = matlabarray(np.zeros((1, length(win1) - 3)))\n        # for n in arange(4, length(win1)).reshape(-1):\n        #     psy1[n - 3] = dot(win1[n - 1], win1[n - 2]) - dot(win1[n], win1[n - 3])\n\n        N=arange(4, length(win1)).reshape(-1)\n        # psy1[1,N - 3] = dot(win1[N - 1], win1[N - 2]) - dot(win1[N], win1[N - 3])\n        psy1[1, N - 3] = win1[N - 1] * win1[N - 2] - win1[N] * win1[N - 3]\n\n        psy1mean = mean(abs(psy1))\n        psy1std = std(psy1, ddof=1)\n\n\n        psy2 = matlabarray(np.zeros((1, length(win2) - 3)))\n\n        # for n in arange(4, length(win2)).reshape(-1):\n        #     psy2[n - 3] = dot(win2[n - 1], win2[n - 2]) - dot(win2[n], win2[n - 3])\n        N=arange(4, length(win2)).reshape(-1)\n        # psy2[N - 3] = dot(win2[N - 1], win2[N - 2]) - dot(win2[N], win2[N - 3])\n        psy2[1, N - 3] = win2[N - 1] * win2[N - 2] - win2[N] * win2[N - 3]\n\n        psy2mean = mean(abs(psy2))\n        psy2std = std(psy2, ddof=1)\n\n\n        psy3 = matlabarray(np.zeros((1, length(win3) - 3)))\n\n        # for n in arange(4, length(win3)).reshape(-1):\n        #     psy3[n - 3] = dot(win3[n - 1], win3[n - 2]) - dot(win3[n], win3[n - 3])\n        N=arange(4, length(win3)).reshape(-1)\n        # psy3[N - 3] = dot(win3[N - 1], win3[N - 2]) - dot(win3[N], win3[N - 3])\n        psy3[1, N - 3] = win3[N - 1] * win3[N - 2] - win3[N] * win3[N - 3]\n\n        psy3mean = mean(abs(psy3))\n        psy3std = std(psy3, ddof=1)\n\n\n        psy4 = matlabarray(np.zeros((1, length(win4) - 3)))\n\n        # for n in arange(4, length(win4)).reshape(-1):\n        #     psy4[n - 3] = dot(win4[n - 1], win4[n - 2]) - dot(win4[n], win4[n - 3])\n        N=arange(4, length(win4)).reshape(-1)\n        # psy4[N - 3] = dot(win4[N - 1], win4[N - 2]) - dot(win4[N], win4[N - 3])\n        psy4[1, N - 3] = win4[N - 1] * win4[N - 2] - win4[N] * win4[N - 3]\n\n        psy4mean = mean(abs(psy4))\n        psy4std = std(psy4, ddof=1)\n\n        line1avg = line1avg + line1\n        line2avg = line2avg + line2\n        line3avg = line3avg + line3\n        line4avg = line4avg + line4\n\n        kurt1avg = kurt1avg + kurt1\n        kurt2avg = kurt2avg + kurt2\n        kurt3avg = kurt3avg + kurt3\n        kurt4avg = kurt4avg + kurt4\n\n        shan1avg = shan1avg + shan1\n        shan2avg = shan2avg + shan2\n        shan3avg = shan3avg + shan3\n        shan4avg = shan4avg + shan4\n\n        psy1meanavg = psy1meanavg + psy1mean\n        psy2meanavg = psy2meanavg + psy2mean\n        psy3meanavg = psy3meanavg + psy3mean\n        psy4meanavg = psy4meanavg + psy4mean\n\n        psy1stdavg = psy1stdavg + psy1std\n        psy2stdavg = psy2stdavg + psy2std\n        psy3stdavg = psy3stdavg + psy3std\n        psy4stdavg = psy4stdavg + psy4std\n\n    line1avg = line1avg / 4\n    line2avg = line2avg / 4\n    line3avg = line3avg / 4\n    line4avg = line4avg / 4\n\n    kurt1avg = kurt1avg / 4\n    kurt2avg = kurt2avg / 4\n    kurt3avg = kurt3avg / 4\n    kurt4avg = kurt4avg / 4\n\n    shan1avg = shan1avg / 4\n    shan2avg = shan2avg / 4\n    shan3avg = shan3avg / 4\n    shan4avg = shan4avg / 4\n\n    psy1meanavg = psy1meanavg / 4\n    psy2meanavg = psy2meanavg / 4\n    psy3meanavg = psy3meanavg / 4\n    psy4meanavg = psy4meanavg / 4\n\n    psy1stdavg = psy1stdavg / 4\n    psy2stdavg = psy2stdavg / 4\n    psy3stdavg = psy3stdavg / 4\n    psy4stdavg = psy4stdavg / 4\n\n    # spectral features\n    #### t1 = ceil(t/Fs/2);\n    tt1 = ceil(t / Fs / 2 - dot(0.5, spectwindow_length))\n    tt2 = ceil(t / Fs / 2 + dot(0.5, spectwindow_length))\n    tt1 = tt1.astype(int)\n    tt2 = tt2.astype(int)\n\n    # edit: matrix select---\n    # spectwin1=spect[:,tt1[1] + 1:tt2[1]]\n    # spectwin2=spect[:,tt1[2] + 1:tt2[2]]\n    # spectwin3=spect[:,tt1[3] + 1:tt2[3]]\n    # spectwin4=spect[:,tt1[4] + 1:tt2[4]]\n    spectwin1 = spect[:, tt1[1]:tt2[1]]\n    spectwin2 = spect[:, tt1[2]:tt2[2]]\n    spectwin3 = spect[:, tt1[3]:tt2[3]]\n    spectwin4 = spect[:, tt1[4]:tt2[4]]\n\n    eps = 0.0\n    delta1 = bandpower(spectwin1, sfreqs, cat(1, 4), 'psd')\n    theta1 = bandpower(spectwin1, sfreqs, cat(4, 8), 'psd')\n    alpha1 = bandpower(spectwin1, sfreqs, cat(8, 12), 'psd')\n    beta1 = bandpower(spectwin1, sfreqs, cat(12, 18), 'psd')\n    total1 = bandpower(spectwin1, sfreqs, cat(1, sfreqs[len(sfreqs) - 2]), 'psd') + eps\n    # total1 = bandpower(spectwin1, sfreqs, cat(1, sfreqs[end()]), 'psd')\n\n    delta1_rat = delta1 / total1\n    theta1_rat = theta1 / total1\n    alpha1_rat = alpha1 / total1\n    beta1_rat = beta1 / total1\n\n    delta_theta1 = delta1 / (theta1 + eps)\n    delta_alpha1 = delta1 / (alpha1 + eps)\n    theta_alpha1 = theta1 / (alpha1 + eps)\n\n    kurtdelta1 = kurtosis(delta1, 0)\n    kurttheta1 = kurtosis(theta1, 0)\n    kurtalpha1 = kurtosis(alpha1, 0)\n    kurtbeta1 = kurtosis(beta1, 0)\n\n    delta1_mean = mean(delta1_rat)\n    theta1_mean = mean(theta1_rat)\n    alpha1_mean = mean(alpha1_rat)\n    beta1_mean = mean(beta1_rat)\n\n    delta1_min = np.min(delta1_rat)\n    theta1_min = np.min(theta1_rat)\n    alpha1_min = np.min(alpha1_rat)\n    beta1_min = np.min(beta1_rat)\n\n    delta1_std = std(delta1_rat)\n    theta1_std = std(theta1_rat)\n    alpha1_std = std(alpha1_rat)\n    beta1_std = std(beta1_rat)\n\n    delta1_prct = prctile(delta1_rat, 95)\n    theta1_prct = prctile(theta1_rat, 95)\n    alpha1_prct = prctile(alpha1_rat, 95)\n    beta1_prct = prctile(beta1_rat, 95)\n\n    delthe1_mean = mean(delta_theta1)\n    delalph1_mean = mean(delta_alpha1)\n    thealph1_mean = mean(theta_alpha1)\n\n    delthe1_min = min(delta_theta1)\n    delalph1_min = min(delta_alpha1)\n    thealph1_min = min(theta_alpha1)\n\n    delthe1_std = std(delta_theta1)\n    delalph1_std = std(delta_alpha1)\n    thealph1_std = std(theta_alpha1)\n\n    delthe1_prct = prctile(delta_theta1, 95)\n    delalph1_prct = prctile(delta_alpha1, 95)\n    thealph1_prct = prctile(theta_alpha1, 95)\n\n    delta2 = bandpower(spectwin2, sfreqs, cat(sfreqs[1], 4), 'psd')\n    theta2 = bandpower(spectwin2, sfreqs, cat(4, 8), 'psd')\n    alpha2 = bandpower(spectwin2, sfreqs, cat(8, 12), 'psd')\n    beta2 = bandpower(spectwin2, sfreqs, cat(12, 18), 'psd')\n    total2 = bandpower(spectwin2, sfreqs, cat(1, sfreqs[len(sfreqs) - 2]), 'psd') + eps\n    # total2=bandpower(spectwin2,sfreqs,'psd') + eps\n    # total2=beta2\n\n    delta2_rat = delta2 / total2\n    theta2_rat = theta2 / total2\n    alpha2_rat = alpha2 / total2\n    beta2_rat = beta2 / total2\n\n    delta_theta2 = delta2 / (theta2 + eps)\n    delta_alpha2 = delta2 / (alpha2 + eps)\n    theta_alpha2 = theta2 / (alpha2 + eps)\n\n    kurtdelta2 = kurtosis(delta2, 0)\n    kurttheta2 = kurtosis(theta2, 0)\n    kurtalpha2 = kurtosis(alpha2, 0)\n    kurtbeta2 = kurtosis(beta2, 0)\n\n    delta2_mean = mean(delta2_rat)\n    theta2_mean = mean(theta2_rat)\n    alpha2_mean = mean(alpha2_rat)\n    beta2_mean = mean(beta2_rat)\n\n    delta2_min = np.min(delta2_rat)\n    theta2_min = np.min(theta2_rat)\n    alpha2_min = np.min(alpha2_rat)\n    beta2_min = np.min(beta2_rat)\n\n    delta2_std = std(delta2_rat)\n    theta2_std = std(theta2_rat)\n    alpha2_std = std(alpha2_rat)\n    beta2_std = std(beta2_rat)\n\n    delta2_prct = prctile(delta2_rat, 95)\n    theta2_prct = prctile(theta2_rat, 95)\n    alpha2_prct = prctile(alpha2_rat, 95)\n    beta2_prct = prctile(beta2_rat, 95)\n\n    delthe2_mean = mean(delta_theta2)\n    delalph2_mean = mean(delta_alpha2)\n    thealph2_mean = mean(theta_alpha2)\n\n    delthe2_min = np.min(delta_theta2)\n    delalph2_min = np.min(delta_alpha2)\n    thealph2_min = np.min(theta_alpha2)\n\n    delthe2_std = std(delta_theta2)\n    delalph2_std = std(delta_alpha2)\n    thealph2_std = std(theta_alpha2)\n\n    delthe2_prct = prctile(delta_theta2, 95)\n    delalph2_prct = prctile(delta_alpha2, 95)\n    thealph2_prct = prctile(theta_alpha2, 95)\n\n    delta3 = bandpower(spectwin3, sfreqs, cat(sfreqs[1], 4), 'psd')\n    theta3 = bandpower(spectwin3, sfreqs, cat(4, 8), 'psd')\n    alpha3 = bandpower(spectwin3, sfreqs, cat(8, 12), 'psd')\n    beta3 = bandpower(spectwin3, sfreqs, cat(12, 18), 'psd')\n    # total3=bandpower(spectwin3,sfreqs,'psd') + eps\n    total3 = bandpower(spectwin3, sfreqs, cat(1, sfreqs[len(sfreqs) - 2]), 'psd') + eps\n\n    delta3_rat = delta3 / total3\n    theta3_rat = theta3 / total3\n    alpha3_rat = alpha3 / total3\n    beta3_rat = beta3 / total3\n\n    delta_theta3 = delta3 / (theta3 + eps)\n    delta_alpha3 = delta3 / (alpha3 + eps)\n    theta_alpha3 = theta3 / (alpha3 + eps)\n\n    kurtdelta3 = kurtosis(delta3, 0)\n    kurttheta3 = kurtosis(theta3, 0)\n    kurtalpha3 = kurtosis(alpha3, 0)\n    kurtbeta3 = kurtosis(beta3, 0)\n\n    delta3_mean = mean(delta3_rat)\n    theta3_mean = mean(theta3_rat)\n    alpha3_mean = mean(alpha3_rat)\n    beta3_mean = mean(beta3_rat)\n\n    delta3_min = np.min(delta3_rat)\n    theta3_min = np.min(theta3_rat)\n    alpha3_min = np.min(alpha3_rat)\n    beta3_min = np.min(beta3_rat)\n\n    delta3_std = std(delta3_rat)\n    theta3_std = std(theta3_rat)\n    alpha3_std = std(alpha3_rat)\n    beta3_std = std(beta3_rat)\n\n    delta3_prct = prctile(delta3_rat, 95)\n    theta3_prct = prctile(theta3_rat, 95)\n    alpha3_prct = prctile(alpha3_rat, 95)\n    beta3_prct = prctile(beta3_rat, 95)\n\n    delthe3_mean = mean(delta_theta3)\n    delalph3_mean = mean(delta_alpha3)\n    thealph3_mean = mean(theta_alpha3)\n\n    delthe3_min = np.min(delta_theta3)\n    delalph3_min = np.min(delta_alpha3)\n    thealph3_min = np.min(theta_alpha3)\n\n    delthe3_std = std(delta_theta3)\n    delalph3_std = std(delta_alpha3)\n    thealph3_std = std(theta_alpha3)\n\n    delthe3_prct = prctile(delta_theta3, 95)\n    delalph3_prct = prctile(delta_alpha3, 95)\n    thealph3_prct = prctile(theta_alpha3, 95)\n\n    delta4 = bandpower(spectwin4, sfreqs, cat(sfreqs[1], 4), 'psd')\n    theta4 = bandpower(spectwin4, sfreqs, cat(4, 8), 'psd')\n    alpha4 = bandpower(spectwin4, sfreqs, cat(8, 12), 'psd')\n    beta4 = bandpower(spectwin4, sfreqs, cat(12, 18), 'psd')\n    # total4=bandpower(spectwin4,sfreqs,'psd') + eps\n    total4 = bandpower(spectwin4, sfreqs, cat(1, sfreqs[len(sfreqs) - 2]), 'psd') + eps\n\n    delta4_rat = delta4 / total4\n    theta4_rat = theta4 / total4\n    alpha4_rat = alpha4 / total4\n    beta4_rat = beta4 / total4\n\n    delta_theta4 = delta4 / (theta4 + eps)\n    delta_alpha4 = delta4 / (alpha4 + eps)\n    theta_alpha4 = theta4 / (alpha4 + eps)\n\n    # kurtdelta4 = kurtosis(delta4);\n    # kurttheta4 = kurtosis(theta4);\n    # kurtalpha4 = kurtosis(alpha4);\n    # kurtbeta4 = kurtosis(beta4);\n    delta4_mean = mean(delta4_rat)\n    theta4_mean = mean(theta4_rat)\n    alpha4_mean = mean(alpha4_rat)\n    beta4_mean = mean(beta4_rat)\n\n    delta4_min = np.min(delta4_rat)\n    theta4_min = np.min(theta4_rat)\n    alpha4_min = np.min(alpha4_rat)\n    beta4_min = np.min(beta4_rat)\n\n    delta4_std = std(delta4_rat)\n    theta4_std = std(theta4_rat)\n    alpha4_std = std(alpha4_rat)\n    beta4_std = std(beta4_rat)\n\n    delta4_prct = prctile(delta4_rat, 95)\n    theta4_prct = prctile(theta4_rat, 95)\n    alpha4_prct = prctile(alpha4_rat, 95)\n    beta4_prct = prctile(beta4_rat, 95)\n\n    delthe4_mean = mean(delta_theta4)\n    delalph4_mean = mean(delta_alpha4)\n    thealph4_mean = mean(theta_alpha4)\n\n    delthe4_min = np.min(delta_theta4)\n    delalph4_min = np.min(delta_alpha4)\n    thealph4_min = np.min(theta_alpha4)\n\n    delthe4_std = std(delta_theta4)\n    delalph4_std = std(delta_alpha4)\n    thealph4_std = std(theta_alpha4)\n\n    delthe4_prct = prctile(delta_theta4, 95)\n    delalph4_prct = prctile(delta_alpha4, 95)\n    thealph4_prct = prctile(theta_alpha4, 95)\n\n    line1avg = line1avg.item()\n    line2avg = line2avg.item()\n    line3avg = line3avg.item()\n    line4avg = line4avg.item()\n\n\n    features1 = matlabarray(\n        cat([line1avg], [line2avg], [line3avg], [line4avg], [kurt1avg], [kurt2avg], [kurt3avg], [kurt4avg]))\n    features1a = matlabarray(\n        cat([shan1avg], [shan2avg], [shan3avg], [shan4avg], [psy1meanavg], [psy2meanavg], [psy3meanavg], [psy4meanavg],\n            [psy1stdavg], [psy2stdavg], [psy3stdavg], [psy4stdavg]))\n    features1aa = matlabarray(\n        cat([delta1_mean], [delta2_mean], [delta3_mean], [delta4_mean], [theta1_mean], [theta2_mean], [theta3_mean],\n            [theta4_mean], [alpha1_mean], [alpha2_mean], [alpha3_mean], [alpha4_mean]))\n    features1b = matlabarray(\n        cat([beta1_mean], [beta2_mean], [beta3_mean], [beta4_mean], [delta1_min], [delta2_min], [delta3_min],\n            [theta1_min], [theta2_min], [theta3_min], [alpha1_min], [alpha2_min], [alpha3_min], [beta1_min],\n            [beta2_min], [beta3_min], [delta1_std], [delta2_std], [delta3_std], [theta1_std], [theta2_std],\n            [theta3_std], [alpha1_std], [alpha2_std], [alpha3_std], [beta1_std], [beta2_std], [beta3_std],\n            [delta1_prct], [delta2_prct], [delta3_prct], [theta1_prct], [theta2_prct], [theta3_prct], [alpha1_prct],\n            [alpha2_prct], [alpha3_prct], [beta1_prct], [beta2_prct], [beta3_prct], [delthe1_mean], [delthe2_mean],\n            [delthe3_mean], [delthe4_mean], [delalph1_mean], [delalph2_mean], [delalph3_mean], [delalph4_mean],\n            [thealph1_mean], [thealph2_mean], [thealph3_mean], [thealph4_mean], [delthe1_min], [delthe2_min],\n            [delthe3_min], [delalph1_min], [delalph2_min], [delalph3_min], [thealph1_min], [thealph2_min],\n            [thealph3_min], [delthe1_std], [delthe2_std], [delthe3_std], [delalph1_std], [delalph2_std], [delalph3_std],\n            [thealph1_std], [thealph2_std], [thealph3_std], [delthe1_prct], [delthe2_prct], [delthe3_prct],\n            [delalph1_prct], [delalph2_prct], [delalph3_prct], [thealph1_prct], [thealph2_prct], [thealph3_prct],\n            [kurtdelta1], [kurtdelta2], [kurtdelta3], [kurttheta1], [kurttheta2], [kurttheta3], [kurtalpha1],\n            [kurtalpha2], [kurtalpha3], [kurtbeta1], [kurtbeta2], [kurtbeta3]))\n    features2 = matlabarray(\n        cat([delta4_min], [theta4_min], [alpha4_min], [beta4_min], [delta4_std], [theta4_std], [alpha4_std],\n            [beta4_std], [delta4_prct], [theta4_prct], [alpha4_prct], [beta4_prct], [delthe4_min], [delalph4_min],\n            [thealph4_min], [delthe4_std], [delalph4_std], [thealph4_std], [delthe4_prct], [delalph4_prct],\n            [thealph4_prct]))\n\n    # kurtbeta4];\n    features = matlabarray(cat(features1, features1a, features1aa, features1b, features2))\n    # print('shape of features',features.shape)\n    # features=matlabarray(cat([features1],[features2]))\n\n    return features\n\n\nclass load_mat():\n    def __init__(self, filename, mode='r'):\n        self.filename = filename\n        self.mode = mode\n\n    def __enter__(self):\n        print('open')\n        # self.open_file = open(self.filename, self.mode)\n        return 0\n\n    def __exit__(self, *args):\n        # self.open_file.close()\n        print('close')\n\ndef load(fp):\n    matfile=hdf5storage.loadmat(fp)\n    return matfile\n\n\n\nif __name__ == '__main__':\n    pass\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}