{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#!pip install --user --upgrade python-swiftclient"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Case105/Case105_seg4.mat\nCase108/Case108_seg10.mat\nCase108/Case108_seg1.mat\nCase108/Case108_seg6.mat\nCase108/Case108_seg7.mat\nCase10/Case10_seg8.mat\nCase110/Case110_seg3.mat\nCase111/Case111_seg7.mat\nCase112/Case112_seg7.mat\nCase113/Case113_seg7.mat\nCase114/Case114_seg1.mat\nCase115/Case115_seg4.mat\nCase117/Case117_seg1.mat\nCase119/Case119_seg4.mat\nCase119/Case119_seg5.mat\nCase11/Case11_seg2.mat\nCase11/Case11_seg3.mat\nCase121/Case121_seg3.mat\nCase122/Case122_seg4.mat\nCase125/Case125_seg2.mat\nCase126/Case126_seg3.mat\nCase127/Case127_seg2.mat\nCase129/Case129_seg1.mat\nCase12/Case12_seg1.mat\nCase130/Case130_seg3.mat\nCase13/Case13_seg2.mat\nCase13/Case13_seg5.mat\nCase13/Case13_seg7.mat\nCase15/Case15_seg4.mat\nCase28/Case28_seg8.mat\nCase31/Case31_seg4.mat\nCase32/Case32_seg3.mat\nCase33/Case33_seg5.mat\nCase35/Case35_seg4.mat\nCase3/Case3_seg6.mat\nCase3/Case3_seg9.mat\nCase41/Case41_seg7.mat\nCase43/Case43_seg6.mat\nCase45/Case45_seg7.mat\nCase48/Case48_seg5.mat\nCase49/Case49_seg9.mat\nCase52/Case52_seg7.mat\nCase53/Case53_seg3.mat\nCase58/Case58_seg3.mat\nCase59/Case59_seg2.mat\nCase63/Case63_seg2.mat\nCase6/Case6_seg3.mat\nCase73/Case73_seg2.mat\nCase75/Case75_seg11.mat\nCase76/Case76_seg9.mat\nCase77/Case77_seg4.mat\nCase86/Case86_seg1.mat\nCase88/Case88_seg2.mat\nCase88/Case88_seg6.mat\nCase89/Case89_seg7.mat\n"
                }
            ], 
            "source": "!find Case*/*.mat -type f -size +300M -size -600M"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "117\n55\n"
                }
            ], 
            "source": "!ls MGH-100P | wc -l\n!find Case*/*.mat -type f -size +300M -size -600M | wc -l"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "compute_spectrogram_sunhaoqi.py  full_pipeline.py  mtspec.py\r\nfcn_shannon_entro.py\t\t get_features.py   runtime.py\r\n"
                }
            ], 
            "source": "#!pip install line_profiler\n#!rm full_pipeline.py\n#!rm get_features.py\n!ls *.py"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "CPU times: user 3.18 ms, sys: 768 \u00b5s, total: 3.94 ms\nWall time: 7.86 s\nCPU times: user 2.95 s, sys: 601 ms, total: 3.55 s\nWall time: 18min 55s\nnum exec 57\n2017-07-21 11:30:08.543619\n"
                }
            ], 
            "source": "%time teraFile = spark.read.text(\"TeraGen-1TB/\")\n%time teraFile.groupBy(\"value\").count().take(100)\nprint('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\nn_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\nimport datetime\n#teraFile.groupBy(\"value\").count().take(100)\nprint(datetime.datetime.now())"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "num exec 57\n"
                }
            ], 
            "source": "print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\nn_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\n\nsc.addPyFile(\"fcn_shannon_entro.py\")\nsc.addPyFile(\"full_pipeline.py\")\nsc.addPyFile(\"compute_spectrogram_sunhaoqi.py\")\nsc.addPyFile(\"get_features.py\")\nsc.addPyFile(\"runtime.py\")\nsc.addPyFile(\"mtspec.py\")"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "num exec 57\nnum data 57\nnum part 57\n"
                }
            ], 
            "source": "# Same bench on full script\nimport itertools\nimport os\nimport numpy as np\nimport hdf5storage\nimport spectrum\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom numpy.core.defchararray import lower\nfrom numpy import floor\nimport swiftclient\nimport pandas as pd\n#from compute_spectrogram_sunhaoqi import mtspecgram_shq\nfrom get_features import get_features\nfrom scipy.signal import butter, filtfilt, lfilter\nfrom runtime import cellarray, matlabarray, cat, cell, arange, size, disp, find, dot, copy, length, Dict2Obj,catstr\nimport scipy\nimport multiprocessing as mp\n\nfrom mtspec import mt_spectrogram\n\n#@contextlib.contextmanager\nclass Timer(object):\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n\n    def __enter__(self):\n        self.start = time.time()\n        return self\n\n    def __exit__(self, *args):\n        self.end = time.time()\n        self.secs = self.end - self.start\n        self.msecs = self.secs * 1000  # millisecs\n        if self.verbose:\n            print('elapsed time: %f ms' % self.msecs)\n\nimport datetime\ndef testFunc(elt):\n        startdt = datetime.datetime.now()\n        try:\n            #d=dict(sc._conf.getAll())\n            #print(d['spark.executor.id'])\n\n            absdir='/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/'\n            elt=absdir+elt\n            datasize=os.path.getsize(elt)\n            matfile = hdf5storage.loadmat(elt)\n\n\n            Fs = cellarray(int(np.round(matfile['Fs'])))\n            start_time = cellarray(matfile['start_time'])\n            channels = cellarray(matfile['channels'])\n            data = cellarray(matfile['data'])\n            \n            cpu_count=mp.cpu_count()\n            Fs = 200\n\n            def strrep_s(a, x, y):\n                return a.replace(x, y)\n\n            strrep = np.vectorize(strrep_s)\n\n            print('channels', channels)\n            print(size(channels, 1))\n            labels = copy(channels)\n\n            for i in arange(1, size(channels, 1)).reshape(-1):\n                x = strrep(channels[i, :], ' ', '')\n                labels[i] = lower(x)\n\n            disp('load 0')\n            disp('compute 1','calculate locational differential voltage')\n            movingwin = matlabarray(cat(2, 2))\n\n            # dictobj\n            params = Dict2Obj({'default': 1})\n\n            params.pad = copy(0)\n            params.fpass = copy(cat(0.5, 20))\n            params.err = copy(0)\n            params.trialave = copy(0)\n            params.tapers = copy(cat(2, 3))\n            params.Fs = copy(200)\n\n            # spect=cell(4,4)\n            # COI=numpy.empty((4, ), dtype=object)\n            COI = cell(4, )\n            COI[1] = cellarray([['fp1', 'f7'], ['f7', 't3'], ['t3', 't5'], ['t5', 'o1']])\n            COI[2] = cellarray([['fp1', 'f3'], ['f3', 'c3'], ['c3', 'p3'], ['p3', 'o1']])\n            COI[3] = cellarray([['fp2', 'f4'], ['f4', 'c4'], ['c4', 'p4'], ['p4', 'o2']])\n            COI[4] = cellarray([['fp2', 'f8'], ['f8', 't4'], ['t4', 't6'], ['t6', 'o2']])\n\n            # test\n            def strcmp_s(x, y):\n                return x == y\n\n            strcmp = np.vectorize(strcmp_s)\n\n            dataB = cell(4, 4)\n\n            # print(data.shape,data.__class__,dataB.shape,dataB.__class__,data.dtype,dataB.dtype)\n            for kk in arange(1, 4).reshape(-1):\n                # print('kk',kk)\n                coi = COI[kk]\n                print('coi', coi, size(coi, 1), size(coi, 2), size(coi))\n\n                for k in arange(1, size(coi, 1)).reshape(-1):\n                    c1 = find(strcmp(labels, coi[k, 1]))\n                    c2 = find(strcmp(labels, coi[k, 2]))\n                    c1 = c1.item()\n                    c2 = c2.item()\n\n                    dataB[kk, k] = np.array((data[c1, :] - data[c2, :]).tolist())    \n\n\n            # Step 2 - spectrogram\n            # Par version\n            import multiprocessing as mp\n            cpu_count=mp.cpu_count()\n            # print(cpu_count)\n            n_jobs=int(np.floor(cpu_count / 2))\n            n_jobs = n_jobs  # number of cpus for parallel computing, -1 is all cpus\n            verbose = True  # verbosity in parallel computing\n            res = Parallel(n_jobs=n_jobs, verbose=verbose)(\n                delayed(mt_spectrogram)(dataB[kk,k], Fs) for kk,k in itertools.product([1,2,3,4],[\n                    1,2,3,4])\n            )\n\n            spect = cell(4, 4)\n\n            for i, rr in enumerate(res):\n                ii=i % 4\n                iii=floor(i/4)\n                spect[iii+1, ii + 1] = rr[0].T\n\n            stimes=res[0][1]\n            sfreqs = res[0][2]\n            \n            msg=spect.shape\n        except Exception as e:\n            print('error')\n            start_time=''\n            data=np.array((0,0))\n            datasize=0\n            \n            cpu_count=0\n            msg=str(e)\n\n        enddt = datetime.datetime.now()\n        dur=enddt-startdt\n        return (elt,start_time,startdt,enddt,datasize,data.shape,cpu_count,dur,msg)\n\n# Benching 8\nrunt=True\nif (runt):\n    #lfiles=!ls Case10*/*.mat\n    #lfiles=!find Case*/*.mat -size -100000\n    # batch with sizes\n    lfiles=!find Case*/*.mat -size +800M -size -1500M\n    \n    print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\n    n_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\n    \n    lsub=lfiles[:n_exec]\n    \n    print('num data',len(lsub))\n    \n    distData = sc.parallelize(lsub)\n    dtd=distData.repartition(n_exec)\n    distData.cache()\n    dtd.cache()\n    \n    print('num part',dtd.getNumPartitions())\n    \n    #testdtd=dtd.map(lambda e: testFunc(e))\n    #testdtd.cache()\n    #%time lres=testdtd.take(len(lsub))    \n    #print(dtd.collect(),lres,len(lres)) \n    "
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "num exec 57\n['Case107/Case107_seg5.mat', 'Case107/Case107_seg3.mat', 'Case108/Case108_seg13.mat', 'Case108/Case108_seg8.mat', 'Case107/Case107_seg9.mat', 'Case109/Case109_seg4.mat', 'Case108/Case108_seg11.mat', 'Case109/Case109_seg5.mat', 'Case108/Case108_seg9.mat', 'Case101/Case101_seg5.mat', 'Case109/Case109_seg2.mat', 'Case101/Case101_seg6.mat', 'Case101/Case101_seg2.mat', 'Case101/Case101_seg4.mat', 'Case102/Case102_seg3.mat', 'Case102/Case102_seg4.mat', 'Case102/Case102_seg2.mat', 'Case102/Case102_seg1.mat', 'Case103/Case103_seg8.mat', 'Case104/Case104_seg2.mat', 'Case103/Case103_seg1.mat', 'Case103/Case103_seg4.mat', 'Case105/Case105_seg1.mat', 'Case105/Case105_seg2.mat', 'Case104/Case104_seg3.mat', 'Case113/Case113_seg10.mat', 'Case104/Case104_seg4.mat', 'Case112/Case112_seg8.mat', 'Case113/Case113_seg11.mat', 'Case113/Case113_seg1.mat', 'Case112/Case112_seg9.mat', 'Case113/Case113_seg3.mat', 'Case113/Case113_seg5.mat', 'Case113/Case113_seg12.mat', 'Case113/Case113_seg13.mat', 'Case10/Case10_seg2.mat', 'Case10/Case10_seg10.mat', 'Case10/Case10_seg4.mat', 'Case10/Case10_seg9.mat', 'Case10/Case10_seg1.mat', 'Case10/Case10_seg6.mat', 'Case110/Case110_seg2.mat', 'Case10/Case10_seg7.mat', 'Case111/Case111_seg3.mat', 'Case111/Case111_seg4.mat', 'Case110/Case110_seg5.mat', 'Case110/Case110_seg4.mat', 'Case112/Case112_seg3.mat', 'Case112/Case112_seg6.mat', 'Case111/Case111_seg6.mat', 'Case112/Case112_seg1.mat', 'Case107/Case107_seg10.mat', 'Case107/Case107_seg2.mat', 'Case106/Case106_seg2.mat', 'Case105/Case105_seg3.mat', 'Case107/Case107_seg8.mat', 'Case107/Case107_seg7.mat'] 57\n"
                }
            ], 
            "source": "print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\nn_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\nn_src=dtd.collect()\nprint(n_src,len(n_src))"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "num exec 57\nnum exec 57\nnum data 57\nnum part 57\nsource files\n['Case109/Case109_seg5.mat', 'Case10/Case10_seg10.mat', 'Case10/Case10_seg9.mat', 'Case110/Case110_seg2.mat', 'Case10/Case10_seg4.mat', 'Case10/Case10_seg6.mat', 'Case111/Case111_seg4.mat', 'Case110/Case110_seg4.mat', 'Case111/Case111_seg6.mat', 'Case111/Case111_seg3.mat', 'Case102/Case102_seg2.mat', 'Case102/Case102_seg3.mat', 'Case101/Case101_seg4.mat', 'Case101/Case101_seg5.mat', 'Case104/Case104_seg4.mat', 'Case104/Case104_seg3.mat', 'Case102/Case102_seg4.mat', 'Case104/Case104_seg2.mat', 'Case107/Case107_seg10.mat', 'Case105/Case105_seg3.mat', 'Case105/Case105_seg1.mat', 'Case105/Case105_seg2.mat', 'Case107/Case107_seg7.mat', 'Case107/Case107_seg8.mat', 'Case107/Case107_seg3.mat', 'Case117/Case117_seg2.mat', 'Case107/Case107_seg5.mat', 'Case118/Case118_seg1.mat', 'Case116/Case116_seg6.mat', 'Case116/Case116_seg7.mat', 'Case118/Case118_seg5.mat', 'Case118/Case118_seg6.mat', 'Case118/Case118_seg7.mat', 'Case118/Case118_seg3.mat', 'Case118/Case118_seg2.mat', 'Case112/Case112_seg6.mat', 'Case112/Case112_seg8.mat', 'Case112/Case112_seg1.mat', 'Case113/Case113_seg12.mat', 'Case112/Case112_seg3.mat', 'Case113/Case113_seg10.mat', 'Case113/Case113_seg1.mat', 'Case113/Case113_seg9.mat', 'Case113/Case113_seg11.mat', 'Case114/Case114_seg2.mat', 'Case113/Case113_seg3.mat', 'Case113/Case113_seg5.mat', 'Case116/Case116_seg4.mat', 'Case116/Case116_seg5.mat', 'Case116/Case116_seg1.mat', 'Case115/Case115_seg1.mat', 'Case109/Case109_seg2.mat', 'Case109/Case109_seg4.mat', 'Case108/Case108_seg13.mat', 'Case108/Case108_seg8.mat', 'Case10/Case10_seg1.mat', 'Case10/Case10_seg2.mat']\n"
                }
            ], 
            "source": "# Same bench on full script\nimport itertools\nimport os\nimport numpy as np\nimport hdf5storage\nimport spectrum\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom numpy.core.defchararray import lower\nfrom numpy import floor\nimport swiftclient\nimport pandas as pd\n#from compute_spectrogram_sunhaoqi import mtspecgram_shq\nfrom get_features import get_features\nfrom scipy.signal import butter, filtfilt, lfilter\nfrom runtime import cellarray, matlabarray, cat, cell, arange, size, disp, find, dot, copy, length, Dict2Obj,catstr\nimport scipy\nimport multiprocessing as mp\n\nfrom mtspec import mt_spectrogram\n\ndef mockFunc(elt):\n    import multiprocessing as mp\n    startdt = datetime.datetime.now()\n    #meas_t=0\n    print('starting',startdt)\n    #try:\n    absdir='/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/'\n    elt=absdir+elt\n    print('process',elt)\n    dataset=os.path.getsize(elt)\n    matfile = hdf5storage.loadmat(elt)\n\n    cpu_count=mp.cpu_count()\n    Fs = cellarray(int(np.round(matfile['Fs'])))\n    start_time = cellarray(matfile['start_time'])\n    channels = cellarray(matfile['channels'])\n    data = cellarray(matfile['data'])\n\n    # New: Resample to 200 Hz!!! %\n    Fs = Fs.item()\n    P = 200\n    Q = int(np.ceil(Fs))\n    print('source freq', Q,'to',P)\n    if Q != P:\n        print('resampling')\n\n        data=scipy.signal.resample(data.T, int(len(data.T) * P / Q))\n        data = cellarray(data.T)\n\n    else:\n        print('skip resampling')\n\n    Fs = 200\n\n    def strrep_s(a, x, y):\n        return a.replace(x, y)\n\n    strrep = np.vectorize(strrep_s)\n\n    print('channels', channels)\n    print(size(channels, 1))\n    labels = copy(channels)\n\n    for i in arange(1, size(channels, 1)).reshape(-1):\n        x = strrep(channels[i, :], ' ', '')\n        labels[i] = lower(x)\n\n    disp('load 0')\n    disp('compute 1','calculate locational differential voltage')\n    movingwin = matlabarray(cat(2, 2))\n\n    # dictobj\n    params = Dict2Obj({'default': 1})\n\n    params.pad = copy(0)\n    params.fpass = copy(cat(0.5, 20))\n    params.err = copy(0)\n    params.trialave = copy(0)\n    params.tapers = copy(cat(2, 3))\n    params.Fs = copy(200)\n\n    # spect=cell(4,4)\n    # COI=numpy.empty((4, ), dtype=object)\n    COI = cell(4, )\n    COI[1] = cellarray([['fp1', 'f7'], ['f7', 't3'], ['t3', 't5'], ['t5', 'o1']])\n    COI[2] = cellarray([['fp1', 'f3'], ['f3', 'c3'], ['c3', 'p3'], ['p3', 'o1']])\n    COI[3] = cellarray([['fp2', 'f4'], ['f4', 'c4'], ['c4', 'p4'], ['p4', 'o2']])\n    COI[4] = cellarray([['fp2', 'f8'], ['f8', 't4'], ['t4', 't6'], ['t6', 'o2']])\n\n    # test\n    def strcmp_s(x, y):\n        return x == y\n\n    strcmp = np.vectorize(strcmp_s)\n\n    dataB = cell(4, 4)\n    for kk in arange(1, 4).reshape(-1):\n        # print('kk',kk)\n        coi = COI[kk]\n        print('coi', coi, size(coi, 1), size(coi, 2), size(coi))\n\n        for k in arange(1, size(coi, 1)).reshape(-1):\n            c1 = find(strcmp(labels, coi[k, 1]))\n            c2 = find(strcmp(labels, coi[k, 2]))\n            c1 = c1.item()\n            c2 = c2.item()\n\n            dataB[kk, k] = np.array((data[c1, :] - data[c2, :]).tolist())    \n\n\n    # Step 2 - spectrogram\n    import multiprocessing as mp\n    n_jobs =int(np.floor(cpu_count*0.7))\n    #n_jobs = -1  # number of cpus for parallel computing, -1 is all cpus\n\n    verbose = True  # verbosity in parallel computing\n    res = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        delayed(mt_spectrogram)(dataB[kk,k], Fs) for kk,k in itertools.product([1,2,3,4],[\n            1,2,3,4])\n    )\n\n    spect = cell(4, 4)\n    #print(len(res))\n\n    for i, rr in enumerate(res):\n        ii=i % 4\n        iii=int(floor(i/4))\n        print(iii + 1, ii + 1)\n        spect[iii+1, ii + 1] = rr[0].T\n\n    stimes = res[0][1]\n    sfreqs = res[0][2]\n\n    # Step 3 - average\n    R = cell(4, 1)\n    for kk in arange(1, 4).reshape(-1):\n        # print('kk',kk,'size',size(spect[kk,1-1]),spect[kk,1-1].shape,tuple(size(spect[kk,1-1])))\n        T = np.zeros(size(spect[kk, 1]))\n\n        print('T', T.shape)\n        for k in arange(1, 4).reshape(-1):\n            # print('kk',kk,'k',k)\n\n            T = T + spect[kk, k]\n        R[kk] = T / 4\n\n\n\n    epoch_length = dot(Fs, 2)\n    window_length = dot(cat(14, 10, 6, 2), Fs)\n\n    # Step 4 -filtering of data\n    fnyq = Fs / 2  # nyquist frequency\n    fc_high = 50\n    fc_low = 0.5\n    #  cutoff frequencies\n    b, a = butter(6, cat(fc_low / fnyq, fc_high / fnyq), 'bandpass')\n\n    for kk in arange(1, 4).reshape(-1):\n        for k in arange(1, 4).reshape(-1):\n            # print('kk',kk,'k',k)\n            temp = dataB[kk, k]\n\n            dataB[kk, k] = filtfilt(b, a, temp, padtype='odd', padlen=3 * (np.amax((len(a), len(b))) - 1))\n\n\n    # Step 5 - features extraction\n    spectwindow_length = floor(window_length / Fs / 2)\n\n    N = length(dataB[1, 1])\n    nr_epochs = floor((N - (window_length[1] - epoch_length) - Fs) / epoch_length)\n    nr_features = 144\n\n    features = cell(4, nr_features, nr_epochs)\n    for kk in arange(1, 4).reshape(-1):\n\n        spect = R[kk, 1]\n        data = dataB[kk, :]\n\n        #Par version\n        #n_jobs = -1 # number of cpus for parallel computing, -1 is all cpus\n        verbose = True  # verbosity in parallel computing\n        res = Parallel(n_jobs=n_jobs, verbose=verbose)(\n            # delayed(compute_spec_each_seg)(eeg[window_start[wi]:window_start[wi] + window_length, :], NW, Fs) for wi in range(window_num)\n            delayed(get_features)(data, int(i.item()), window_length, spect, sfreqs, spectwindow_length, Fs) for i\n            in arange(1, nr_epochs).reshape(-1)\n        )\n\n        print(len(res))\n        for i, rr in enumerate(res):\n            features[kk, :, i + 1] = rr[:]\n\n    features = features.reshape(cat(dot(4, nr_features), length(features)))\n    features = features.T            \n\n\n    # Step 6\n    fileName0=elt\n    saveFeats = features.T\n\n    # turn this into a pandas df\n    saveFeats = pd.DataFrame(saveFeats)\n\n    # add time stamp\n    saveFeats.reset_index(inplace=True)\n    fileNum = fileName0[:len(fileName0) - 4]\n    import re\n    fileNum = re.search('(\\d+)$', fileNum).group(0)\n    sampleRate = round(Fs)\n\n    def timeStamp(ind, fileNum, sampleRate):\n        start = (int(fileNum) - 1) * 99999\n        row = start + int(ind)\n        time = row * int(sampleRate)\n        return time\n\n    saveFeats['time'] = saveFeats['index'].apply(lambda e: timeStamp(e, fileNum, sampleRate))\n\n    # add case id\n    caseNum = int(re.search(r'\\d+', fileName0).group())\n    saveFeats['case'] = caseNum\n\n    try:\n        print(fileName0)\n        onlyFiName = os.path.basename(fileName0)\n        fp = os.path.basename(fileName0) + '.csv'\n        print(fp)\n        savep = absdir + 'MGH-100P/new_features_' + fp\n        print('save to', savep)\n        if not os.path.isfile(savep):\n            saveFeats.to_csv(savep)\n\n    except:\n        print('[ERROR]', 'saving to csv')\n\n\n\n    ## get credential\n    #objStorCred = {\n    #  \"auth_url\": \"https://identity.open.softlayer.com\",\n    #  \"project\": \"object_storage_20ff227d_d66c_495a_a316_cd99e80a9e6f\",\n    #  \"projectId\": \"a9fb4d478e3d40a8bbd54c5a2ecf25a3\",\n    #  \"region\": \"dallas\",\n    #  \"userId\": \"6a4cc8251c1940179a6cccc9098a15e0\",\n    #  \"username\": \"admin_fd35793e7cf915d9a5a9c768b068029cf6d720b9\",\n    #  \"password\": \"kDTcKA2H(3eo5.G0\",\n    #  \"domainId\": \"a350f0fe7fb44571b29305706a12c95a\",\n    #  \"domainName\": \"1334933\",\n    #  \"role\": \"admin\"\n    #}\n    #conn = swiftclient.Connection(auth_version='3',key=objStorCred['password'],\n    #                          authurl=objStorCred['auth_url']+'/v3',\n    #                         os_options={\"project_id\":objStorCred['projectId'],\n    #                                   \"user_id\":objStorCred['userId'],\n    #                                   \"region_name\":objStorCred['region']})            \n    #    \n    ## copy the local csv to obj storage\n    #try:\n    #    container = 'MGH'\n    #    print('transfer to',)\n    #    with open(savep, 'r') as local:\n    #        # conn.put_object(container, \"Case\"+str(caseNum)+\"/\"+onlyFiName+\".csv\",contents=local,content_type='text/plain')\n    #        conn.put_object(container, onlyFiName + \".csv\", contents=local, content_type='text/plain')\n    #except:\n    #    print('[ERROR]', 'saveint to object store')        \n\n    #Parallelize spectrogram\n    #n_jobs=-1\n    #verbose=10\n    #res = Parallel(n_jobs=n_jobs, verbose=verbose)(\n    #   delayed(mt_spectrogram)(cellarray(matfile['data'][i,:]), Fs) for i in range(1,16))\n    msg='No issues'\n    #except Exception as e:\n    #    print('error')\n    #    #msg=str(e)\n    #    #print('error')\n    #    Fs=''\n    #    start_time=''\n    #    dataset=(0,0)\n    #    dataB=(0,0)\n    #    msg=str(e) \n    #    datasize=0\n    #    meas_t=0\n    #    #dataset=dataset.shape\n        \n        \n        ## Parallelize filtering\n        #out = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        #   delayed(filtering_data)(data[i],Fs) for i in range(16))\n\n    enddt = datetime.datetime.now()\n    dur = enddt-startdt\n    return (elt,start_time,startdt,enddt,datasize,data.shape,cpu_count,dur,msg)\n\nrunt=True\nif (runt):\n    #!ls MGH-100P\n    \n    print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\n    n_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\n    \n    #lsub=['Case1/Case1_seg12.mat','Case109/Case109_seg1.mat','Case89/Case89_seg3.mat']\n    #lsub=['Case17/Case17_seg7.mat','Case17/Case17_seg3.mat','Case11/Case11_seg4.mat','Case11/Case11_seg4.mat']\n   # \n    #lsub=['Case11/Case11_seg4.mat','Case11/Case11_seg4.mat','Case11/Case11_seg4.mat','Case11/Case11_seg4.mat']\n    \n    lfiles=!find Case*/*.mat -type f -size +900M -size -1500M\n    \n    \n    print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\n    n_exec=sc._jsc.sc().getExecutorMemoryStatus().size()\n    \n    lsub=lfiles[:n_exec]\n    \n    print('num data',len(lsub))\n    \n    distData = sc.parallelize(lsub)\n    dtd=distData.repartition(n_exec)\n    distData.cache()\n    dtd.cache()\n    \n    print('num part',dtd.getNumPartitions())\n    \n    #testdtd=dtd.map(lambda e: mockFunc(e))\n    #testdtd.cache()\n    #%time lres=testdtd.collect()\n    \n    print('source files')\n    print(dtd.collect())"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "57"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dtd.count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "num exec 57\n"
                }
            ], 
            "source": "print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\nn_exec=sc._jsc.sc().getExecutorMemoryStatus().size()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "2017-07-21 11:30:19.129404\n"
                }
            ], 
            "source": "import datetime\nprint(datetime.datetime.now())\n\ntestdtd=dtd.map(lambda e: mockFunc(e))\n#testdtd=dtd.map(lambda e: testFunc(e))\n#testdtd.cache()\n#%time lres=testdtd.take(len(lsub))\n%time lres=testdtd.collect()\n#%time lsrc=dtd.collect()\n#print(lsrc,len(lsrc))\nprint(lres,len(lres)) \n\nprint(datetime.datetime.now())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import numpy as np\noutput = np.array(lres)\nfor line in output[np.argsort(output[:,2])]:\n    print(line[2],line[3],line[8])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "1 == 2"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Test Func\nrunt=False\nif (runt):\n    import datetime\n    print(datetime.datetime.now())\n\n    #testdtd=dtd.map(lambda e: mockFunc(e))\n    testdtd=dtd.map(lambda e: testFunc(e))\n    #testdtd.cache()\n    #%time lres=testdtd.take(len(lsub))\n    %time lres=testdtd.collect()\n    #%time lsrc=dtd.collect()\n    #print(lsrc,len(lsrc))\n    print(lres,len(lres)) \n\n    print(datetime.datetime.now())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "    print('num exec',sc._jsc.sc().getExecutorMemoryStatus().size())\n    n_exec=sc._jsc.sc().getExecutorMemoryStatus().size()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import datetime\nprint(datetime.datetime.now())\n\ntestdtd=dtd.map(lambda e: mockFunc(e))\n#testdtd=dtd.map(lambda e: testFunc(e))\n#testdtd.cache()\n#%time lres=testdtd.take(len(lsub))\n%time lres=testdtd.collect()\n#%time lsrc=dtd.collect()\n#print(lsrc,len(lsrc))\nprint(lres,len(lres)) \n\nprint(datetime.datetime.now())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\noutput = np.array(lres)\nfor line in output[np.argsort(output[:,2])]:\n    print(line[2],line[3],line[8])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!echo $HOME"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!ls $SPARK_CONFIG_HOME"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import os  \nos.listdir(os.environ['SPARK_CONFIG_HOME'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "def readSparkConfig(spark):  \n    \"\"\"\n    A function to read the Spark Config of DSX\n    :param spark: a string specifying which `spark-defaults.conf` to read. \n    :return: a dict, where the keys corespond to the parameters of `spark-defaults.conf` and the values are the values.\n    \"\"\"\n    if not spark in os.listdir(os.environ['SPARK_CONFIG_HOME']):\n        #print \"'%s' is not a valid option- valid options are: \" % (spark, \", \".join(os.listdir(os.environ['SPARK_CONFIG_HOME'])))\n        return None\n    with open(os.environ['SPARK_CONFIG_HOME'] + \"/%s/spark-defaults.conf\" % spark, 'r') as f:\n        spark_conf = f.read()\n    spark_conf_dict = {line.split( )[0] : line.split( )[1] for line in spark_conf.split(\"\\n\") if len(line.split( )) > 0}\n    return spark_conf_dict"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "test = readSparkConfig('spark21master-python3') "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "test['spark.files'] = '/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/full_pipeline.py,/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/get_features.py,/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/fcn_shannon_entro.py,/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/mtspec.py,/gpfs/fs01/user/seff-34c2f0d3dcc620-a916a00b641d/notebook/work/runtime.py'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!pwd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "test"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "def writeSparkConfig(spark, spark_conf_dict):  \n    \"\"\"\n    A function to write the updated Spark Config of DSX (overwrites previous config)\n    :param spark: a string specifying which `spark-defaults.conf` to write. \n    :return: None \n    \"\"\"\n\n    if not spark in os.listdir(os.environ['SPARK_CONFIG_HOME']):\n        #print \"'%s' is not a valid option- valid options are: \" % (spark, \", \".join(os.listdir(os.environ['SPARK_CONFIG_HOME'])))\n        return None\n    with open(os.environ['SPARK_CONFIG_HOME'] + \"/%s/spark-defaults.conf\" % spark, 'w') as f:\n        for k,v in spark_conf_dict.items():\n            print(k + \"\\t\" + v + \"\\n\")\n            ln=k + \"\\t\" + v + \"\\n\"\n            f.write(ln)\n    #print \"Successfully wrote new configuration to %s\" % (os.environ['SPARK_CONFIG_HOME'] + \"/%s/spark-defaults.conf\" % spark)\n    return None"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "writeSparkConfig('spark21master-python3', test)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!cp -rf $SPARK_CONFIG_HOME/spark21master-python3 $SPARK_CONFIG_HOME/spark21master-python3_bu; ls $SPARK_CONFIG_HOME"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!printenv"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}